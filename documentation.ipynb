{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv, dotenv_values\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running aiR for Review\n",
      "aiR for Review uses generative AI to simulate the actions of a human reviewer, finding and describing relevant documents using the review instructions that you provide. These analyses can be customized to search for relevance, key documents, or specific issues as needed.\n",
      "The instructions you give aiR for Review are called Prompt Criteria. For best results, we recommend analyzing a small set of documents, tweaking the Prompt Criteria as needed, then finally analyzing a larger set of documents. This lets you see immediately how aiR's coding compares to a human reviewer's coding and adjust the prompts accordingly.\n",
      "Note: aiR for Review is currently in limited release. For information about the general release, contact your account representative.\n",
      "See these related pages:\n",
      "\n",
      "aiR for Review\n",
      "\n",
      "Monitoring aiR for Review jobs\n",
      "\n",
      "aiR for Review results\n",
      "\n",
      "aiR for Review security permissions\n",
      "\n",
      "\n",
      "See these additional resources:\n",
      "\n",
      "Workflows for Applying aiR for Review\n",
      "\n",
      "aiR for Review example project\n",
      "\n",
      "\n",
      "Installing aiR for Review\n",
      "aiR for Review is available as a secured application from the Application Library. You must have an active aiR for Review contract to use it, and it is not available for repository workspaces.\n",
      "To install it:\n",
      "\n",
      "\n",
      "Navigate to the Relativity Applications tab in your workspace.\n",
      "\n",
      "\n",
      "Select Install from application library.\n",
      "\n",
      "\n",
      "Select the aiR for Review application.\n",
      "\n",
      "\n",
      "Click Install.\n",
      "\n",
      "\n",
      "After installation completes, the following object types will appear in your workspace:\n",
      "\n",
      "\n",
      "aiR Relevance Analysis—records the Relevance results of aiR for Review analysis runs.\n",
      "\n",
      "\n",
      "aiR Issue Analysis—records the Issue results of aiR for Review analysis runs.\n",
      "\n",
      "\n",
      "aiR Key Analysis—records the Key results of aiR for Review analysis runs.\n",
      "\n",
      "\n",
      "aiR for Review Prompt Criteria—records the Prompt Criteria settings and contents for each analysis run. This also records Prompt Criteria drafts for each user.\n",
      "\n",
      "\n",
      "Installing aiR for Review creates two versions of the aiR for Review Jobs tab: one at the instance level, and one at the workspace level. Some users also find it helpful to manually create tabs for the three Analysis objects, but this is optional.\n",
      "For more information on installing applications, see Relativity applications.\n",
      "Setting up permissions\n",
      "For detailed information on aiR for Review user permissions, see aiR for Review security permissions.\n",
      "Choosing an analysis type\n",
      "aiR for Review supports three types of analysis. Each one is geared towards a different phase of a review or investigation.\n",
      " For each aiR for Review job, choose one analysis type:\n",
      "\n",
      "\n",
      "Relevance—analyzes whether documents are relevant to a case or situation that you describe, such as documents responsive to a production request.\n",
      "\n",
      "\n",
      "Relevance and Key Documents—analyzes documents for both relevance and whether they are “hot” or key to a case.\n",
      "\n",
      "\n",
      "Issues—analyzes documents for whether they include content that falls under specific categories. For example, you might use this to check whether documents involve coercion, retaliation, or a combination of both.\n",
      "\n",
      "\n",
      "Based on the analysis type you choose, you will need the following fields:\n",
      "\n",
      "\n",
      "Relevance—one single-choice results field. The field must have at least one choice.\n",
      "\n",
      "\n",
      "Relevance and Key Documents—two single-choice results fields. These should have distinct names such as \"Relevant\" and \"Key,\" and each field should have at least one choice.\n",
      "\n",
      "\n",
      "Issues—one multi-choice results field. Each choice should represent one of the issues you want to analyze.\n",
      "Note: Currently, aiR for Review analyzes a maximum of five issues per run. You can have as many choices for the field as you want, but you can only analyze five at a time.\n",
      "\n",
      "\n",
      "aiR for Review does not actually write to these fields. Instead, it uses them as reference when making and reporting on its predictions.\n",
      "Choosing a processing mode\n",
      "aiR for Review supports two processing modes: Fast Track and Batch. These are designed for the workflow of fine-tuning Prompt Criteria on a small set of documents, then using the Prompt Criteria on a large set of documents.\n",
      "Because the large language model (LLM) has limited capacity, we dedicate some bandwidth exclusively to Fast Track jobs. This returns speedy results for smaller jobs without them having to wait for larger jobs in the queue.\n",
      " For each aiR for Review job, choose one mode:\n",
      "\n",
      "\n",
      "Fast Track—processes up to 50 documents quickly. Use this to test and refine your Prompt Criteria on a small set of test documents.\n",
      "\n",
      "\n",
      "Each user can have only one Fast Track job running at a time.\n",
      "\n",
      "\n",
      "These jobs use dedicated bandwidth that is not available to larger batch jobs. They typically return results within 5-10 minutes.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batch—processes up to 50,000 documents. Use this to run your previously refined Prompt Criteria on a larger set of documents.\n",
      "\n",
      "\n",
      "Each instance can have up to three Batch jobs running at a time.\n",
      "\n",
      "\n",
      "Processing time varies based on document load and total load on the LLM.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "For more detailed information about each mode’s capacity, see Job capacity and size limitations.\n",
      "Best practices for running aiR for Review\n",
      "aiR for Review works best after fine-tuning the Prompt Criteria. Analyzing just a few documents at first, comparing the results to human coding, and then adjusting the Prompt Criteria as needed yields more accurate results than diving in with a full document set.\n",
      "We recommend the following workflow:\n",
      "\n",
      "\n",
      "For your first analysis, run the Prompt Criteria on a set of 10 test documents that are a mix of relevant and  not relevant.\n",
      "\n",
      "\n",
      "Compare the results to human coding. In particular, look for documents that aiR coded differently than the humans did and investigate possible reasons. This could include unclear instructions, needing to define an acronym or code word, or other blind spots in the Prompt Criteria.\n",
      "\n",
      "\n",
      "Tweak the Prompt Criteria to adjust for blind spots.\n",
      "\n",
      "\n",
      "Repeat steps 1 through 3 until aiR predicts coding decisions accurately for the test documents.\n",
      "\n",
      "\n",
      "Test the Prompt Criteria on 50 documents and compare results. Continue tweaking as needed.\n",
      "\n",
      "\n",
      "Finally, run the Prompt Criteria on a larger set of documents.\n",
      "\n",
      "\n",
      "aiR only sees the extracted text of a document. It does not see any non-text elements like advanced formatting, embedded images, or videos. We do not recommend using aiR for Review on documents such as images, videos, or spreadsheets with heavy formulas. Instead, use it on documents whose extracted text accurately represents their content and meaning.\n",
      "Running the analysis\n",
      "aiR for Review works as a mass action found on the Documents tab. Running the analysis has three basic parts:\n",
      "\n",
      "\n",
      "Selecting documents and setting up the review\n",
      "\n",
      "\n",
      "Writing the Prompt Criteria\n",
      "\n",
      "\n",
      "Submitting the job for analysis\n",
      "\n",
      "\n",
      "At any point in this process, you can click Save and Close in the mass action modal. This saves your progress so that you can keep working on it at a later time.\n",
      "When you reopen the mass action modal, the last Prompt Criteria that you saved will display. For more information, see Running aiR for Review.\n",
      "Step 1: Selecting documents and setting up the review\n",
      "To start an aiR for Review analysis job:\n",
      "\n",
      "\n",
      "From the Documents tab, select the documents you want to analyze.\n",
      "\n",
      "\n",
      "Under Mass Actions, select aiR for Review. A modal with several tabs appears.\n",
      "\n",
      "\n",
      "On the Setup tab of the modal, set the following:\n",
      "\n",
      "\n",
      "Prompt Criteria Name—give the Prompt Criteria a unique name. You can also click Load Prompt Criteria to select Prompt Criteria that you or another user previously wrote. For more information, see Editing and collaboration.\n",
      "\n",
      "\n",
      "Review Type—select one of the following. For more information, see Choosing an analysis type.\n",
      "\n",
      "Relevance—analyzes whether documents are relevant to a case or situation that you describe, such as documents responsive to a production request.\n",
      "\n",
      "Relevance and Key Documents—analyzes documents for both relevance and whether they are “hot” or key to a case.\n",
      "\n",
      "\n",
      "Issues—analyzes documents for whether they include content that falls under specific categories.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Run in Fast Track—toggle this On for 50 documents or fewer, and Off for more than 50 documents. For more information, see Choosing a processing mode.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The first time you use the mass action, all the fields will be blank. When you click Save and Next or Save and Close, your progress saves and persists for the next analysis.\n",
      "If any required fields on any of the tabs are empty or misconfigured, the Save and Next button will be unavailable. Click on the title of each tab to fill out its fields.\n",
      "Step 2: Writing the Prompt Criteria\n",
      "The Prompt Criteria are a set of inputs that give aiR the context it needs to understand the matter and evaluate each document. Writing the Prompt Criteria is a way of training your \"reviewer,\" similar to training a human reviewer.\n",
      "Depending which type of analysis you chose, you will see a different set of tabs. All Prompt Criteria include the Case Summary tab.\n",
      "General writing guidelines\n",
      "For all of the setup tabs, we recommend:\n",
      "\n",
      "\n",
      "Write as if \"less is more.\" Instead of pasting in a long review protocol as-is, summarize where possible and include only key passages. The Prompt Criteria have an overall length limit of 10,000 characters.\n",
      "\n",
      "\n",
      "Phrase things in a positive way when possible. Avoid negatives (\"not\" statements) and double negatives.\n",
      "\n",
      "\n",
      "Do not include explanations of the law.\n",
      "\n",
      "\n",
      "Do not give the LLM commands, such as “you will review XX.\" Instead, simply describe the case.\n",
      "\n",
      "\n",
      "Use whatever writing format makes the most sense to a human reader. For example, bullet points might be useful for the People and Aliases section, but paragraphs might make sense in another section.\n",
      "\n",
      "\n",
      "The LLM has essentially “read the whole Internet.” It understands widely used slang and abbreviations, but it does not necessarily know jargon or phrases that are specific to an organization.\n",
      "\n",
      "\n",
      "When you start to write your first Prompt Criteria, the fields contain grayed-out helper text that shows examples of what to enter. Use this as a guideline for crafting your own entries.\n",
      "Note: \n",
      "For more guidance on prompt writing, see the following resources on the Community site:\n",
      "\n",
      "\n",
      "aiR for Review Prompt Writing Best Practices—downloadable PDF of writing guidelines\n",
      "\n",
      "\n",
      "aiR for Review example project—detailed example of adapting a review protocol into Prompt Criteria\n",
      "\n",
      "\n",
      "\n",
      "Filling out the Case Summary tab\n",
      "The Case Summary gives the broad context surrounding a matter. It includes an overview of the matter, people and entities involved, and any jargon or terms that are needed to understand the document set.\n",
      "Limit the Case Summary content to roughly 20 sentences overall, and 20 each of People and Aliases, Noteworthy Organizations, and Noteworthy Terms.\n",
      "To fill out the Case Summary tab:\n",
      "\n",
      "\n",
      "Within the setup modal, click on the Case Summary tab.\n",
      "\n",
      "\n",
      "Fill out the following:\n",
      "\n",
      "\n",
      "Matter Overview—provide a concise overview of the case. Include the names of the plaintiff and defendant, the nature of the dispute, and other important case characteristics.\n",
      "\n",
      "\n",
      "People and Aliases—list the names and aliases of key custodians who authored or received the documents. Include their role and any other affiliations.\n",
      "\n",
      "\n",
      "Noteworthy Organizations—list the organizations and other relevant entities involved in the case. Highlight any key relationships or other notable characteristics.\n",
      "\n",
      "\n",
      "Noteworthy Terms—list and define any relevant words, phrases, acronyms, jargon, or slang that might be important to the analysis.\n",
      "\n",
      "\n",
      "Additional Context—list any additional information that does not fit the other fields. This section is typically left blank.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Depending on which Review Type you chose, the remaining tabs will be called Relevance, Key Documents, or Issues. Fill out those tabs according to the guide sections below.\n",
      "Filling out the Relevance tab\n",
      "If you chose either Relevance or Relevance and Key Documents as the Review Type, you will see the Relevance tab. This defines the fields and criteria used for determining if a document is relevant to the case.\n",
      "To fill out the Relevance tab:\n",
      "\n",
      "\n",
      "Within the setup modal, click on the Relevance tab.\n",
      "\n",
      "\n",
      "Fill out the following:\n",
      "\n",
      "\n",
      "Relevance Field—select a single-choice field that represents whether a document is relevant or non-relevant.\n",
      "\n",
      "\n",
      "Relevant Choice—select the field choice you use to mark a document as relevant.\n",
      "\n",
      "\n",
      "Relevance Criteria—summarize the criteria that determine whether a document is relevant. Include:\n",
      "\n",
      "\n",
      "Keywords, phrases, legal concepts, parties, entities, and legal claims\n",
      "\n",
      "\n",
      "Any criteria that would make a document non-relevant, such as relating to a project that is not under dispute\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Issues Field (Optional)—select a single-choice or multi-choice field that represents the issues in the case.\n",
      "\n",
      "\n",
      "Choice Criteria—select each of the field choices one by one. For each choice, write a summary in the text box listing the criteria that determine whether that issue applies to a document. For more information, see Filling out the Issues tab.\n",
      "\n",
      "\n",
      "Note: aiR does not make Issue predictions during Relevance review, but you can use this field for reference when writing the Relevance Criteria. For example, you could tell aiR that any documents related to these issues are relevant.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "For best results when writing the Relevance Criteria:\n",
      "\n",
      "\n",
      "Limit the Relevance Criteria to 5-10 sentences.\n",
      "\n",
      "\n",
      "Do not paste in the original request for production (RFP); those are often too long and complex to give good results. Instead, summarize it and include key excerpts.\n",
      "\n",
      "\n",
      "Group similar criteria together when you can. For example, if an RFP asks for “emails pertaining to X” and “documents pertaining to X,” write “emails or documents pertaining to X.”\n",
      "\n",
      "\n",
      "Filling out the Key Documents tab\n",
      "If you chose Relevance and Key as the Review Type, you will see the Key Documents tab. This defines the fields and criteria used for determining if a document is \"hot\" or key to the case.\n",
      "To fill out the Key Documents tab:\n",
      "\n",
      "\n",
      "Within the setup modal, click on the Key Documents tab.\n",
      "\n",
      "\n",
      "Fill out the following:\n",
      "\n",
      "\n",
      "Key Document Field—select a single-choice field that represents whether a document is key to the case.\n",
      "\n",
      "\n",
      "Key Document Choice—select the field choice you use to mark a document as key.\n",
      "\n",
      "\n",
      "Key Document Criteria—summarize the criteria that determine whether a document is key. Include:\n",
      "\n",
      "\n",
      "Keywords, phrases, legal concepts, parties, entities, and legal claims\n",
      "\n",
      "\n",
      "Any criteria that would exclude a document from being key, such as falling outside a certain date range\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "For best results, limit the Key Document Criteria to 5-10 sentences.\n",
      "Filling out the Issues tab\n",
      "If you chose Issues as the Review Type, you will see the Issues tab. This defines the fields and criteria used for determining whether a document relates to a set of specific topics or issues.\n",
      "To fill out the Issues tab:\n",
      "\n",
      "\n",
      "Within the setup modal, click on the Issues tab.\n",
      "\n",
      "\n",
      "Fill out the following:\n",
      "\n",
      "\n",
      "Field—select a multi-choice field that represents the issues in the case.\n",
      "\n",
      "\n",
      "Choice Criteria—select each of the field choices one by one. For each choice, write a summary in the text box listing the criteria that determine whether that issue applies to a document. Include:\n",
      "\n",
      "\n",
      "Keywords, phrases, legal concepts, parties, entities, and legal claims\n",
      "\n",
      "\n",
      "Any criteria that would exclude a document from being key, such as falling outside a certain date range\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "For best results when writing the Choice Criteria:\n",
      "\n",
      "\n",
      "Limit the criteria description for each choice to 5-10 sentences.\n",
      "\n",
      "\n",
      "Each of the choices must have its own criteria. If a choice has no criteria, either fill it in or remove the choice.\n",
      "\n",
      "\n",
      "Removing issue choices\n",
      "aiR analyzes a maximum of 5 choices. If the issue field has more than 5 choices:\n",
      "\n",
      "\n",
      "Select the choice you want to remove.\n",
      "\n",
      "\n",
      "Click the Remove Choice button on the right.\n",
      "\n",
      "\n",
      "Repeat with any other unwanted choices.\n",
      "\n",
      "\n",
      "Step 3: Submitting the job for analysis\n",
      "After filling out the Setup, Case Summary, and other tabs, review the job and submit it for analysis.\n",
      "To submit a job:\n",
      "\n",
      "\n",
      "Click Save and Next.\n",
      "\n",
      "\n",
      "Review the confirmation summary. This includes:\n",
      "\n",
      "\n",
      "Total Docs—number of documents to be analyzed.\n",
      "\n",
      "\n",
      "Est. Total Doc Units—number of document units counted for billing purposes. For more information, see How document units are calculated.\n",
      "\n",
      "\n",
      "Est. Time to Start—estimated wait time from when you submit the job, to when aiR can begin analyzing your job. Longer wait times appear when aiR already has other work queued up.\n",
      "\n",
      "\n",
      "Est. Run Time—estimated time aiR will take to analyze and return the results of the documents selected. This does not include time waiting in the queue.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Click Start Classification.\n",
      "A banner appears showing that the job was successfully submitted for analysis.\n",
      "\n",
      "\n",
      "Note: If you try to run a job that is too large or when too many jobs are already running, an error will appear. You can still save and edit the Prompt Criteria, but you will not be able to start the job. For more information, see Job capacity and size limitations.\n",
      "For information on monitoring jobs in progress, see Monitoring aiR for Review jobs.\n",
      "Editing and collaboration\n",
      "By default, when you select the mass action, the last Prompt Criteria you saved will display. This makes it easy to edit the Prompt Criteria without re-entering information.\n",
      "If you want to edit a different set of Prompt Criteria or collaborate with another user, you can load previous Prompt Criteria into the mass action modal. From there, any edits you make will be saved as a new set of Prompt Criteria.\n",
      "To load previous Prompt Criteria:\n",
      "\n",
      "\n",
      "From the Documents tab, check the documents you want to analyze.\n",
      "\n",
      "\n",
      "Under Mass Actions, select aiR for Review.\n",
      "\n",
      "\n",
      "On the Setup tab of the modal,  click Load Prompt Criteria. A pop-up opens with two tabs:\n",
      "\n",
      "\n",
      "Prompt Criteria—Prompt Criteria for jobs that already ran in the workspace.\n",
      "\n",
      "\n",
      "Drafts—each user’s most recently saved Prompt Criteria. These may or may not have been run yet.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Select a row from either tab.\n",
      "The right-hand panel shows a preview of the Prompt Criteria.\n",
      "\n",
      "\n",
      "Click Load.\n",
      "The Prompt Criteria Name, Analysis Type, and all criteria load into the aiR for Review modal.\n",
      "\n",
      "\n",
      "If you run the previous Prompt Criteria without making any changes, the Prompt Criteria Name stays the same. If you edit the prompt before running it, a number such as (1) or (2) will be automatically added to the end of the Prompt Criteria Name. You can also manually enter your own name for the Prompt Criteria as you edit it.\n",
      "If you and another user both edit the same Prompt Criteria at the same time, your edits are saved as separate drafts. To collaborate on the same draft, we recommend having the first user finish their edits, then pass the draft off to the second user.\n",
      "Note: Only one Prompt Criteria draft is saved for each user. If you save a draft, then load in different Prompt Criteria, that draft will be overwritten. To save Prompt Criteria long-term, run them with one or more documents.\n",
      "How document units are calculated\n",
      "A document unit is a document with between 1 and 15,000 characters of text. If a document has more than 15,000 characters in it, it is counted as two or more document units. For example, a 1-character document counts as one document unit, but a 16,000-character document counts as two document units.\n",
      "Note: Any Unicode character counts as one character, regardless of storage size.\n",
      "Because the document unit estimation calculates white space slightly differently than actual billing, small discrepancies may appear for document sizes that are right on the border between two document units. To find the actual document units that are billed, see the Cost Explorer.\n",
      "Job capacity and size limitations\n",
      "Based on the limits of the underlying large language model (LLM), aiR has size limits for the documents and prompts you submit, as well as volume limits for the overall jobs.\n",
      "Size limits\n",
      "The documents and Prompt Criteria have the following size limits:\n",
      "\n",
      "\n",
      "The Prompt Criteria have an overall length limit of 10,000 characters.\n",
      "\n",
      "\n",
      "Each document's extracted text should be under 150KB if possible. aiR has a hard limit of 300KB extracted text per document, but due to how the LLM processes document size, it sometimes rejects documents that are smaller than 300KB. To avoid this, we recommend treating 150KB as the upper limit.\n",
      "\n",
      "\n",
      "Each document's extracted text, when combined with the Prompt Criteria, must be less than 32,000 “tokens” (roughly equivalent to words, symbols, or whitespace). This is usually not a problem for documents under 150KB.\n",
      "\n",
      "\n",
      "Volume limits\n",
      "The volume limits for aiR for Review jobs are as follows:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Volume Type\n",
      "Limit\n",
      "Notes\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Max job size for Fast Track mode\n",
      "50 documents\n",
      "For over 50 documents, use Batch mode.\n",
      "\n",
      "\n",
      " Max job size for Batch mode \n",
      "50,000 documents\n",
      "\n",
      "A single Batch job can include up to 50,000 documents.\n",
      "\n",
      "\n",
      "\n",
      "Concurrent Fast Track jobs per user\n",
      "1 job\n",
      "Each user can only have one Fast Track job running at a time. However, multiple users in one instance can run Fast Track jobs at the same time.\n",
      "\n",
      "\n",
      "Concurrent Batch jobs per instance\n",
      "3 jobs\n",
      "Only 3 Batch jobs can be queued or running at the same time within an instance.\n",
      "\n",
      "\n",
      "\n",
      "Speed\n",
      "After a job is submitted, aiR analyzes roughly 25-50 documents per minute. Fast Track jobs typically take under 10 minutes. Batch job speeds vary widely depending on the number of documents, the overall load on the LLM, and other factors.\n",
      "\n",
      "Monitoring aiR for Review jobs\n",
      "After an aiR for Review job has started, you can use the aiR for Review jobs tab to monitor its progress, view prompt details, or cancel it. You can also view completed jobs and choose which analysis results are connected to the documents.\n",
      "For Fast Track jobs, banner statuses appear on the Documents tab to let you know when a job is complete. You can also monitor these from the aiR for Review jobs tab like any other job.\n",
      "Note: aiR for Review is currently in limited release. For information about the general release, contact your account representative.\n",
      "See these related pages:\n",
      "\n",
      "aiR for Review\n",
      "\n",
      "Running aiR for Review\n",
      "\n",
      "aiR for Review results\n",
      "\n",
      "aiR for Review security permissions\n",
      "\n",
      "\n",
      "aiR for Review Jobs tab\n",
      "There are two versions of the aiR for Review Jobs tab: one at the instance level, and one at the workspace level. The instance-level tab shows all jobs across all workspaces, and it includes several extra columns to identify the workspace, matter, and client connected to each job. Most users only need access to the workspace-level tab. However, because some of aiR's volume limits are instance-wide, the instance-level tab makes it easy to see exactly how much capacity is being used.\n",
      "Both versions of the tab show aiR for Review jobs that have been submitted for analysis. You can use the tab to view prompt details, cancel queued or in-progress jobs, and manage the job results.\n",
      "For information on managing tab permissions, see aiR for Review security permissions.\n",
      "Note: If the aiR for Review Jobs tab says that aiR for Review is not currently available, check with your administrator. Your organization might not have an active contract for aiR for Review.\n",
      "Managing jobs and document linking\n",
      "You can use the aiR for Review Jobs tab to cancel jobs, clear job results out of the document fields, and restore previous job results.\n",
      "To manage jobs, use the following icons:\n",
      "\n",
      "\n",
      "Cancel symbol (\n",
      ")—cancels a queued or in-progress job. Any results that were already received from the large language model (LLM) will stay in the fields, and those results will still be billed.\n",
      "\n",
      "\n",
      "Clear symbol (\n",
      ")—clears job results from the documents in this run. This empties the aiR for Review fields and removes highlighting from the Viewer, but it does not permanently delete the results. The results can be restored and re-linked at any time.\n",
      "\n",
      "\n",
      "Restore symbol (\n",
      ")—re-links the results of the selected job to the documents in the run. This replaces the results of any other job with the same result type.\n",
      "\n",
      "\n",
      "For example, if you realize your current Prompt Criteria gives you less helpful results than a previous Prompt Criteria did, you can restore the previous job's results. This immediately gives reviewers access to the old predictions without needing to re-run the old Prompt Criteria.\n",
      "Notes: \n",
      "If you run a new job on documents that were part of a previous job, you may temporarily see both sets of results linked to those documents. The old results will be unlinked after the new job is complete.\n",
      "\n",
      "To avoid seeing doubled results, clear the previous result set using the aiR for Review Jobs tab.\n",
      "\n",
      "\n",
      "Viewing job details\n",
      "To see an aiR for Review job's Prompt Criteria, click on its row. A detail panel opens showing the setup details, case summary, fields, and criteria for analysis.\n",
      "You can control a user's access to the detail panel using both item-level and workspace-level permissions. For more information, see aiR for Review security permissions.\n",
      "Jobs tab fields\n",
      "The following fields appear on the aiR for Review Jobs tab:\n",
      "\n",
      "\n",
      "Job ID—the unique ID assigned to a job.\n",
      "\n",
      "\n",
      "Prompt Criteria Name—the name of the Prompt Criteria used by the job.\n",
      "\n",
      "\n",
      "If several jobs ran using the same Prompt Criteria, this name will be the same for those jobs.\n",
      "\n",
      "\n",
      "If a user edited the Prompt Criteria before running the job but did not change the name, the Prompt Criteria name will have a version number such as (1) or (2) appended after it.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Job Status—the current state of the job. The possible statuses are:\n",
      "\n",
      "\n",
      "Not Started\n",
      "\n",
      "\n",
      "Queued\n",
      "\n",
      "\n",
      "In Progress\n",
      "\n",
      "\n",
      "Completed\n",
      "\n",
      "\n",
      "Cancelling\n",
      "\n",
      "\n",
      "Errored\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Job Type—the job's processing mode. For more information, see Choosing a processing mode.\n",
      "\n",
      "\n",
      "Client Name (workspace-level only)—the client associated with the job's workspace.\n",
      "\n",
      "\n",
      "Matter Name (workspace-level only)—the matter name associated with the job's workspace.\n",
      "\n",
      "\n",
      "Matter Number (workspace-level only)—the matter number associated with the job's workspace.\n",
      "\n",
      "\n",
      "Workspace ID (workspace-level only)—the ID of the job's workspace.\n",
      "\n",
      "\n",
      "Workspace Name (workspace-level only)—the name of the job's workspace.\n",
      "\n",
      "\n",
      "Doc Count—the number of documents submitted for analysis.\n",
      "\n",
      "\n",
      "Docs Successful—the number of documents that were successfully analyzed.\n",
      "\n",
      "\n",
      "Docs Pending—the number of documents that are waiting to be analyzed.\n",
      "\n",
      "\n",
      "Docs Errored—the number of documents that encountered an error during analysis.\n",
      "\n",
      "\n",
      "Docs Skipped—the number of documents that aiR did not return results for. This can happen for reasons such as cancelling a job, network errors, and partial or complete job failures.\n",
      "\n",
      "\n",
      "User Name—the user who submitted the job.\n",
      "\n",
      "\n",
      "Submitted Time—the time the user submitted the job.\n",
      "\n",
      "\n",
      "Completed Time—the time the job successfully completed. If the job failed or was cancelled early, this field is blank.\n",
      "\n",
      "\n",
      "Terminated Time—the time the job stopped running, regardless of whether it was cancelled, failed, or completed successfully.\n",
      "\n",
      "\n",
      "Job Failure Reason—if the job failed, the reason is listed here. If the job completed successfully, this field is blank.\n",
      "\n",
      "\n",
      "Estimated Wait Time—the initial estimate for how long the job will wait between when the user submits the job and when the job can start running.\n",
      "\n",
      "\n",
      "Estimated Run Time—the initial estimate for how long the job will take to run.\n",
      "\n",
      "\n",
      "Document Units—the number of documents counted for billing purposes. For more information, see How document units are calculated.\n",
      "\n",
      "\n",
      "Fast Track banner statuses\n",
      "When you submit a Fast Track job, the confirmation banner on the Documents tab updates automatically to show you the status of the job. This makes it easier to keep track of a small job's status without needing to check the dedicated Jobs tab. This banner only appears for the user who submitted the job.\n",
      "The banner updates when the job is queued, in progress, and complete.\n",
      " \n",
      "\n",
      "aiR for Review results\n",
      "When aiR for Review analyzes documents, it makes predictions about the relevance of documents to different topics or issues. If it predicts that a document is relevant or relates to an issue, it includes a written justification of that prediction, as well as a counterargument and in-text citations. You can view these predictions, citations, and justifications either from the Viewer, or from a custom document view.\n",
      "Note: aiR for Review is currently in limited release. For information about the general release, contact your account representative.\n",
      "See these related pages:\n",
      "\n",
      "aiR for Review\n",
      "\n",
      "Running aiR for Review\n",
      "\n",
      "Monitoring aiR for Review jobs\n",
      "\n",
      "aiR for Review security permissions\n",
      "\n",
      "\n",
      "How aiR for Review results work\n",
      "When aiR for Review finishes its analysis, it returns a prediction about how each document should be categorized, as well as its reasons for that prediction. This analysis has several parts:\n",
      "\n",
      "\n",
      "Rank—a numerical rank that indicates how strongly relevant the document is or how well it matches the predicted issue.\n",
      "\n",
      "\n",
      "Prediction—the relevance, key, or issue label that aiR predicts should apply to the document.\n",
      "\n",
      "\n",
      "Rationale—an explanation of why aiR chose this rank and prediction.\n",
      "\n",
      "\n",
      "Considerations—a counterargument explaining why the rationale might possibly be wrong.\n",
      "\n",
      "\n",
      "Citations—excerpts from the document that support the prediction and rationale.\n",
      "\n",
      "\n",
      "In general, citations are left empty for non-relevant documents and documents that don't match an issue. However, aiR occasionally provides a citation for low-ranking documents if it helps to clarify why it was marked non-relevant. For example, if aiR is searching for changes of venue, it might cite an email that ends with \"Hang on, gotta run, more later\" as worth noting, even though it understands that’s not a true change of venue request.\n",
      "Predictions versus document coding\n",
      "Even though aiR refers to the relevance, key, and issue fields during its analysis, it does not actually write to these fields. All of aiR's results are stored in aiR-specific fields such as the Prediction field. We recommend using these aiR fields for reference and reserving the actual relevance, key, and issue fields for human coding.\n",
      "For ideas on how to integrate aiR for Review results into a larger review workflow, see Using aiR for Review with Review Center.\n",
      "Variability of results\n",
      "Because of how large language models work, results can vary slightly from run to run. aiR's results for an individual document can potentially change even when given the same set of inputs. However, this is relatively rare; from our testing, it happens about 4% of the time.\n",
      "Understanding document ranks\n",
      "aiR ranks documents from 0 to 4 according to how relevant they are or how well they match an issue. The higher the number, the more relevant the document is predicted to be. In addition, aiR assigns a rank of -1 to any errored documents. Because these were not properly analyzed, they cannot receive a normal rank.\n",
      "The aiR for Review ranks are:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Rank\n",
      "Description\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-1\n",
      "The document either encountered an error or could not be analyzed. For more information, see How document errors are handled.\n",
      "\n",
      "\n",
      "0 \n",
      "The document is “junk” data such as system files or sets of random characters.\n",
      "\n",
      "\n",
      "1\n",
      "The document is predicted not relevant. aiR did not find any evidence that it relates to the case or issue.\n",
      "\n",
      "\n",
      "2\n",
      "The document is predicted borderline relevant. aiR found some content that might relate to the case or issue. It usually has citations.\n",
      "\n",
      "\n",
      "3\n",
      "The document is predicted relevant to the issue. Citations show the relevant text.\n",
      "\n",
      "\n",
      "4\n",
      "The document is predicted very relevant to the issue. aiR found direct, strong evidence that the content relates to the case or issue. Citations show the relevant text.\n",
      "\n",
      "\n",
      "\n",
      "Viewing results for individual documents\n",
      "From the Viewer, you can see the aiR for Review results for each individual document. Predictions show up in the left-hand pane, and all citations are automatically highlighted.\n",
      "To view a document's aiR for Review results, click on the aiR for Review Analysis icon () to expand the pane. The aiR for Review Analysis pane displays the following:\n",
      "\n",
      "\n",
      "Analysis Name\n",
      "\n",
      "\n",
      "Prediction\n",
      "\n",
      "\n",
      "Rationale and Considerations\n",
      "\n",
      "\n",
      "Citation\n",
      "\n",
      "\n",
      "Notes: \n",
      "If you run a new job on documents that were part of a previous job, you may temporarily see both sets of results linked to those documents. The old results will be unlinked after the new job is complete.\n",
      "\n",
      "To avoid seeing doubled results, clear the previous result set using the aiR for Review Jobs tab.\n",
      "\n",
      "\n",
      "Citations and highlighting\n",
      "To jump to a specific citation, click the citation card. You can also toggle highlighting on or off by clicking the toggle at the top of the aiR for Review Analysis pane.\n",
      "The highlight colors depend on the type of citation:\n",
      "\n",
      "\n",
      "Relevance citation—orange.\n",
      "\n",
      "\n",
      "Key Document citation—purple.\n",
      "\n",
      "\n",
      "Issue citation—color set chosen in the Color Map application. For more information, see Changing the color associated with a coding choice.\n",
      "\n",
      "\n",
      "If the same passage is cited by two types of results, the highlight blends their colors.\n",
      "Adding aiR for Review fields to layouts\n",
      "Because of how aiR for Review results fields are structured, you cannot add them directly to layouts. If the highlighting is not enough, you can add an object list to the layout that shows all linked results. For more information, see Adding and editing an object list.\n",
      "Viewing results for groups of documents\n",
      "You can view and compare aiR for Review results for large groups of documents by adding their fields to document views and saved searches.\n",
      "Each field name is formatted as aiR <review type> Analysis::<fieldname>. For example, the Prediction field for a Relevance analysis is called aiR Relevance Analysis::Prediction.\n",
      "For a full field list, see aiR for Review results fields.\n",
      "Notes: \n",
      "If you run a new job on documents that were part of a previous job, you may temporarily see both sets of results linked to those documents. The old results will be unlinked after the new job is complete.\n",
      "\n",
      "To avoid seeing doubled results, clear the previous result set using the aiR for Review Jobs tab.\n",
      "\n",
      "\n",
      "Creating an aiR for Review results view\n",
      "When creating a view for aiR for Review results, we recommend including these fields:\n",
      "\n",
      "\n",
      "Edit\n",
      "\n",
      "\n",
      "Control Number\n",
      "\n",
      "\n",
      "<Review Field>\n",
      "\n",
      "\n",
      "aiR <Review Type> Analysis::Rank\n",
      "\n",
      "\n",
      "aiR <Review Type> Analysis::Prediction\n",
      "\n",
      "\n",
      "Because the Rationale, Citation, and Considerations fields have larger blocks of text, those tend to be less helpful for comparing many documents. However, you can also add those if desired.\n",
      "For a full field list, see aiR for Review results fields.\n",
      "Filtering and sorting aiR for Review results\n",
      "Documents have a one-to-many relationship with the aiR for Review's results fields. For example, a single document might be linked to five Issue results. This creates some limitations when sorting and filtering results:\n",
      "\n",
      "\n",
      "Filter one column at a time in the Document list. Combining filters may include more results than you expect.\n",
      "\n",
      "\n",
      "If you need to filter by more than one field at a time, we recommend using search conditions instead.\n",
      "\n",
      "\n",
      "You can add these fields to views and widgets, but you cannot sort the view or the widget by these fields.\n",
      "\n",
      "\n",
      "aiR for Review results fields\n",
      "The results of every aiR for Review analysis are stored as part of an analysis object. Each of the three result types has its own object type to match:\n",
      "\n",
      "\n",
      "aiR Relevance Analysis\n",
      "\n",
      "\n",
      "aiR Key Analysis\n",
      "\n",
      "\n",
      "aiR Issue Analysis\n",
      "\n",
      "\n",
      "aiR also links the results to each of the documents that were analyzed. These linked fields, called reflected fields, update to link to the newest results every time the document is analyzed. However, aiR keeps a record of all previous job results, and you can link the documents to a different job at any time. For more information, see Managing jobs and document linking.\n",
      "The reflected fields are the most useful for reviewing analysis results. These are formatted as aiR <review type> Analysis::<fieldname>. For example, the Prediction field for a Relevance analysis is called aiR Relevance Analysis::Prediction.\n",
      "aiR Relevance Analysis fields\n",
      "The fields for aiR Relevance Analysis are:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Field name\n",
      "Field type\n",
      "Description\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Name\n",
      "\n",
      "\n",
      "Fixed-length Text\n",
      "\n",
      "\n",
      "The name of this specific result. This formatted as <Document Artifact ID>_<Job ID>.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Job ID\n",
      "\n",
      "\n",
      "Fixed-length Text\n",
      "\n",
      "\n",
      "The unique ID of the job this result came from.\n",
      "\n",
      "\n",
      "\n",
      "Rank\n",
      "Whole Number\n",
      "Numerical rank indicating how strongly relevant the document is. For more information, see Understanding document ranks.\n",
      "\n",
      "\n",
      "Document\n",
      "Multiple Object\n",
      "The Control Number of the document this result is linked to. If the result is not currently linked to any documents, this field is blank.\n",
      "\n",
      "\n",
      "Prediction\n",
      "Fixed-length Text\n",
      "aiR's prediction of whether this qualifies as a relevant document.\n",
      "\n",
      "\n",
      "Rationale\n",
      "Fixed-length Text\n",
      "An explanation of why aiR chose this rank and prediction.\n",
      "\n",
      "\n",
      "Considerations\n",
      "Fixed-length Text\n",
      "A counterargument explaining why the rationale might possibly be wrong.\n",
      "\n",
      "\n",
      "Citation 1\n",
      "Fixed-length Text\n",
      "Excerpt from the document that supports the prediction and rationale. This may be blank for some documents.\n",
      "\n",
      "\n",
      "Citation 2\n",
      "Fixed-length Text\n",
      "Second excerpt from the document that supports the prediction and rationale. This may be blank for some documents.\n",
      "\n",
      "\n",
      "Citation 3\n",
      "Fixed-length Text\n",
      "Third excerpt from the document that supports the prediction and rationale. This may be blank for some documents.\n",
      "\n",
      "\n",
      "Citation 4\n",
      "Fixed-length Text\n",
      "Fourth excerpt from the document that supports the prediction and rationale. This may be blank for some documents.\n",
      "\n",
      "\n",
      "Citation 5\n",
      "Fixed-length Text\n",
      "Fifth excerpt from the document that supports the prediction and rationale. This may be blank for some documents.\n",
      "\n",
      "\n",
      "Error Details\n",
      "Fixed-length Text\n",
      "If the document encountered an error, the error message displays here. For an error list, see How document errors are handled.\n",
      "\n",
      "\n",
      "\n",
      "aiR Issues Analysis fields\n",
      "The fields for aiR Issues Analysis are:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Field name\n",
      "Field type\n",
      "Description\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Name\n",
      "\n",
      "\n",
      "Fixed-length Text\n",
      "\n",
      "\n",
      "The name of this specific result. This formatted as <Document ID>_<Job ID>.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Job ID\n",
      "\n",
      "\n",
      "Fixed-length Text\n",
      "\n",
      "\n",
      "The unique ID of the job this result came from.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Choice Analyzed\n",
      "\n",
      "\n",
      "Fixed Text\n",
      "\n",
      "\n",
      "The name of the issue choice being analyzed for this result.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Choice Analyzed ID\n",
      "\n",
      "\n",
      "Whole Number\n",
      "\n",
      "\n",
      "The Artifact ID of the issue choice being analyzed for this\n",
      " result.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Document\n",
      "\n",
      "\n",
      "Multiple Object\n",
      "\n",
      "\n",
      "The Control Number of the document this result is linked to. If the result is not currently linked to any documents, this field is blank.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Rank\n",
      "\n",
      "\n",
      "Whole Number\n",
      "\n",
      "\n",
      "Numerical rank indicating how well the document matches an issue. For more information, see Understanding document ranks.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Prediction\n",
      "\n",
      "\n",
      "Fixed-length Text\n",
      "\n",
      "\n",
      "aiR's predicted issue choice for this document.\n",
      "\n",
      "\n",
      "\n",
      "Rationale\n",
      "Fixed-length Text\n",
      "An explanation of why aiR chose this rank and prediction.\n",
      "\n",
      "\n",
      "Considerations\n",
      "Fixed-length Text\n",
      "A counterargument explaining why the rationale might possibly be wrong.\n",
      "\n",
      "\n",
      "\n",
      "Citation\n",
      "\n",
      "\n",
      "Fixed-length Text\n",
      "\n",
      "\n",
      "Excerpt from the document that supports the prediction and rationale. This may be blank for some documents.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Error Details\n",
      "\n",
      "\n",
      "Fixed-length Text\n",
      "\n",
      "\n",
      "If the document encountered an error, the error message displays here. For an error list, see How document errors are handled.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "aiR Key Analysis fields\n",
      "The fields for aiR Key Analysis are:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Field name\n",
      "Field type\n",
      "Description\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Name\n",
      "\n",
      "\n",
      "Fixed-length Text\n",
      "\n",
      "\n",
      "The name of this specific result. This formatted as <Document ID>_<Job ID>.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Job ID\n",
      "\n",
      "\n",
      "Fixed-length Text\n",
      "\n",
      "\n",
      "The unique ID of the job this result came from.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Document\n",
      "\n",
      "\n",
      "Multiple Object\n",
      "\n",
      "\n",
      "The Control Number of the document this result is linked to. If the result is not currently linked to any documents, this field is blank.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Rank\n",
      "\n",
      "\n",
      "Whole Number\n",
      "\n",
      "\n",
      "Numerical rank indicating how strongly relevant the document is. For more information, see Understanding document ranks.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Prediction\n",
      "\n",
      "\n",
      "Fixed-length Text\n",
      "\n",
      "\n",
      "aiR's prediction of whether this qualifies as a key document.\n",
      "\n",
      "\n",
      "\n",
      "Rationale\n",
      "Fixed-length Text\n",
      "An explanation of why aiR chose this rank and prediction.\n",
      "\n",
      "\n",
      "Considerations\n",
      "Fixed-length Text\n",
      "A counterargument explaining why the rationale might possibly be wrong.\n",
      "\n",
      "\n",
      "Citation 1\n",
      "Fixed-length Text\n",
      "Excerpt from the document that supports the prediction and rationale. This may be blank for some documents.\n",
      "\n",
      "\n",
      "Citation 2\n",
      "Fixed-length Text\n",
      "Second excerpt from the document that supports the prediction and rationale. This may be blank for some documents.\n",
      "\n",
      "\n",
      "Citation 3\n",
      "Fixed-length Text\n",
      "Third excerpt from the document that supports the prediction and rationale. This may be blank for some documents.\n",
      "\n",
      "\n",
      "Citation 4\n",
      "Fixed-length Text\n",
      "Fourth excerpt from the document that supports the prediction and rationale. This may be blank for some documents.\n",
      "\n",
      "\n",
      "Citation 5\n",
      "Fixed-length Text\n",
      "Fifth excerpt from the document that supports the prediction and rationale. This may be blank for some documents.\n",
      "\n",
      "\n",
      "\n",
      "Error Details\n",
      "\n",
      "\n",
      "Fixed-length Text\n",
      "\n",
      "\n",
      "If the document encountered an error, the error message displays here. For an error list, see How document errors are handled.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Using aiR for Review with Review Center\n",
      "One option for integrating aiR for Review into a larger review workflow is to combine it with Review Center. After analyzing the documents with aiR for Review, you can use aiR's predictions to prioritize which documents to include in a Review Center queue.\n",
      "For example, you may want to review all documents that aiR for Review ranked as borderline or above for relevance. To do that:\n",
      "\n",
      "\n",
      "Set up a saved search for documents where aiR Relevance Analysis::Rank is greater than 1. This returns all documents ranked 2 or higher.\n",
      "\n",
      "\n",
      "Create a Review Center queue using that saved search as the data source.\n",
      "\n",
      "\n",
      "Because of how the aiR for Review fields are structured, you cannot sort by them. However, you can either sort by another field, or use a prioritized review queue to dynamically serve up documents that may be most relevant.\n",
      "For more information, see Review Center.\n",
      "How document errors are handled\n",
      "If aiR encounters a problem when analyzing a document, it will not return results for that document. Instead, it ranks the document as -1 and returns an error message in the Error Details column. Your organization is not charged for any errored documents.\n",
      "The possible error messages are:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Error message\n",
      "Description\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Failed to parse completion \n",
      "The large language model (LLM) encountered an error. \n",
      "\n",
      "\n",
      "Completion is not valid JSON \n",
      "The large language model (LLM) encountered an error.\n",
      "\n",
      "\n",
      "Hallucination detected in completion \n",
      "The results for this document may include a hallucination. For more information, see Hallucinations and conflations.\n",
      "\n",
      "\n",
      "Conflation detected in completion \n",
      "The results for this document may include a conflation. For more information, see Hallucinations and conflations.\n",
      "\n",
      "\n",
      "Document text is empty \n",
      "The extracted text of the document was empty.\n",
      "\n",
      "\n",
      "Document text is too short \n",
      "There was not enough extracted text to analyze in the document.\n",
      "\n",
      "\n",
      "Document text is too long \n",
      "The document's extracted text was too long to analyze.\n",
      "\n",
      "\n",
      "Model API error occurred \n",
      "A communication error occurred between the large language model (LLM) and Relativity.\n",
      "\n",
      "\n",
      "Uncategorized error occurred \n",
      "An unknown error occurred.\n",
      "\n",
      "\n",
      "\n",
      "Hallucinations and conflations\n",
      "Two types of errors deserve special mention:\n",
      "\n",
      "\n",
      "Hallucinations—these occur when the aiR results citation cannot be found anywhere in the prompt text. The large language model (LLM) appears to be citing sentences that don't exist in the prompt.\n",
      "\n",
      "\n",
      "Conflations—these occur when the aiR results citation comes from something other than the document itself, but which is still part of the full prompt. For example, it might cite text that was part of the Prompt Criteria instead of the document's extracted text.\n",
      "\n",
      "\n",
      "When aiR receives the analysis results from the LLM, it checks all citations against the prompt text. Any possible hallucinations or conflations are marked as errors, and they receive a rank of -1 instead of whatever rank they were originally assigned. We recommend manually reviewing errored documents.\n",
      "Hallucinations are typically rare. However, highly structured documents such as Excel spreadsheets and PDF forms have a higher hallucination rate than other document types.\n",
      "Note: Due to the way that columns of text are scanned in OCR, OCR’d documents are occasionally marked as hallucinations when the citations do actually exist in the original document.\n",
      "\n",
      "aiR for Review security permissions\n",
      "This page contains information on the security permissions required for interacting with aiR for Review. For more information on setting permissions, see Workspace security.\n",
      "Note: aiR for Review is currently in limited release. For information about the general release, contact your account representative.\n",
      "See these related pages:\n",
      "\n",
      "aiR for Review\n",
      "\n",
      "Running aiR for Review\n",
      "\n",
      "Monitoring aiR for Review jobs\n",
      "\n",
      "aiR for Review results\n",
      "\n",
      "\n",
      "Running the aiR for Review mass action\n",
      "To run the aiR for Review mass action, you need the following permissions:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Object Security\n",
      "Other Settings\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "aiR for Review Profile - View, Edit, Add\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Mass Operations - aiR for Review\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "You must also belong to at least one user group other than the Workspace Admin Group.\n",
      "Viewing the aiR for Review Jobs tab\n",
      "There are two versions of the aiR for Review Jobs tab: one at the instance level, and one at the workspace level. The instance-level tab shows all jobs across all workspaces, and it includes several extra columns to identify the workspace, matter, and client connected to each job.\n",
      "The following permissions allow users to see the job list and click on each job to view Prompt Criteria details. Users with access to this tab can also cancel in-progress jobs.\n",
      "Instance-level permissions\n",
      "To view the instance-level aiR for Review Jobs tab, you need the following permissions:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Object Security\n",
      "Tab Visibility \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Admin Repository - View\n",
      "aiR for Review Prompt Criteria - View\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "aiR for Review Jobs\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Assign these permissions under the Instance Details tab.\n",
      "Viewing Prompt Criteria at the instance level\n",
      "To view Prompt Criteria details for a job, you also need some permissions within that job's workspace:\n",
      "\n",
      "You must belong to more than just the Workspace Admin Group within the workspace.\n",
      "You must have aiR for Review Prompt Criteria - View rights within that job's workspace.\n",
      "\n",
      "If you do not have these, you will be able to see jobs from that workspace, but you will not be able click on those jobs to view their Prompt Criteria.\n",
      "You can also use item-level permissions to restrict access to a specific job's aiR for Review Prompt Criteria. For more information, see Levels of Security in Relativity.\n",
      "Workspace-level permissions\n",
      "To view the workspace-level aiR for Review Jobs tab, you need the following permissions:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Object Security\n",
      "Tab Visibility \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "aiR for Review Prompt Criteria - View\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "aiR for Review Jobs\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Assign these permissions under the Workspace Details tab within the chosen workspace.\n",
      "You can also use item-level permissions to restrict access to a specific job's aiR for Review Prompt Criteria. For more information, see Levels of Security in Relativity.\n",
      "Clearing and restoring job results\n",
      "To clear or restore job results using the aiR for Review Jobs tab, you need the following permissions:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Object Security\n",
      "Tab Visibility \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Document - View, Edit\n",
      "aiR Relevance Analysis - View, Edit\n",
      "aiR Issue Analysis - View, Edit\n",
      "aiR Key Analysis - View, Edit\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "aiR for Review Jobs\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "If you have Edit permissions for only one of the analysis types, you will only be able to clear or restore results of that type.\n",
      "For more information on clearing and restoring results, see Managing jobs and document linking.\n",
      "Viewing highlights in the Viewer\n",
      "To  see aiR for Review results highlighted in the Viewer, you need the following permissions:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Object Security\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "aiR Relevance Analysis - View\n",
      "aiR Issue Analysis - View\n",
      "aiR Key Analysis - View\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "If you only have access to some of these, you will only see highlighting for those analysis types.\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# URL of the page you want to scrape\n",
    "base_url = \"https://help.relativity.com/RelativityOne/Content/Relativity/aiR_for_Review/aiR_for_Review.htm\"\n",
    "llm_input = ''\n",
    "\n",
    "# Send a GET request to the webpage\n",
    "response = requests.get(base_url)\n",
    "\n",
    "# If the GET request is successful, the status code will be 200\n",
    "if response.status_code == 200:\n",
    "    # Get the content of the response\n",
    "    content = response.content\n",
    "\n",
    "    # Create a BeautifulSoup object and specify the parser\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "    # Find the sub-menu element\n",
    "    sub_menu = soup.find(class_='guideTOC')\n",
    "\n",
    "    # Find all anchor tags within the sub-menu and extract href attributes\n",
    "    links = [urljoin(base_url, a['href']) for a in sub_menu.find_all('a', href=True)]\n",
    "\n",
    "    # Loop through the links\n",
    "    for link in links:\n",
    "        # Send a GET request to the child page\n",
    "        response = requests.get(link)\n",
    "\n",
    "        # If the GET request is successful, the status code will be 200\n",
    "        if response.status_code == 200:\n",
    "            # Get the content of the response\n",
    "            content = response.content\n",
    "\n",
    "            # Create a BeautifulSoup object and specify the parser\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "            # Find the specified div tags\n",
    "            paragraphs = soup.find_all('div', id=\"mc-main-content\")\n",
    "\n",
    "            # Loop through the paragraphs, get the text of each one, and print it\n",
    "            for paragraph in paragraphs:\n",
    "                llm_input += paragraph.get_text()\n",
    "        else:\n",
    "            print(\"Failed to retrieve the webpage. Status code:\", response.status_code)\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage. Status code:\", response.status_code)\n",
    "\n",
    "print(llm_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To run aiR for Review, you need the following permissions:\n",
      "\n",
      "1. **aiR for Review Profile - View, Edit, Add**: This permission allows you to interact with aiR for Review, including setting up, editing, and adding new profiles.\n",
      "\n",
      "2. **Mass Operations - aiR for Review**: This permission enables you to run mass operations related to aiR for Review, such as analyzing documents, setting up prompt criteria, and submitting jobs for analysis.\n",
      "\n",
      "Make sure you also belong to at least one user group other than the Workspace Admin Group for the system to recognize and authorize you to run aiR for Review.\n"
     ]
    }
   ],
   "source": [
    "#Pass the documentation text to GPT-3.5 Turbo and ask a question.\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": f\"You are a teacher helping me understand the following documentation on the aiR for Review product: {llm_input}\"},\n",
    "    {\"role\": \"user\", \"content\": f\"What permissions are needed to run aiR for Review?\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kw-ds-testing-dC1OrvnL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
