{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv, dotenv_values\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape all links on the main doc page, then grab the content from each link as well as the sub-links within them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve the webpage. Status code: 401\n",
      "Failed to retrieve the webpage. Status code: 404\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_and_save(url, directory='results', depth=1):\n",
    "    # Send a GET request to the webpage\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # If the GET request is successful, the status code will be 200\n",
    "    if response.status_code == 200:\n",
    "        # Get the content of the response\n",
    "        content = response.content\n",
    "\n",
    "        # Create a BeautifulSoup object and specify the parser\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "        # Find the main content\n",
    "        main_content = soup.find('div', id=\"mc-main-content\")\n",
    "\n",
    "        # Save the main content to a file\n",
    "        if main_content:\n",
    "            with open(os.path.join(directory, f'{urlparse(url).path.split(\"/\")[-1]}.txt'), 'w', encoding='utf-8') as f:\n",
    "                f.write(main_content.get_text())\n",
    "\n",
    "        # Initialize links as an empty list\n",
    "        links = []\n",
    "\n",
    "        # If depth is 1, find normal href links\n",
    "        if depth == 2:\n",
    "            links = [urljoin(url, a['href']) for a in soup.find_all('a', href=True) if not a['href'].startswith(('javascript:', 'tel:'))]\n",
    "        # If depth is greater than 1, look for the sub-menu\n",
    "        elif depth > 0:\n",
    "            # Find the sub-menu element\n",
    "            sub_menu = soup.find(class_='guideTOC')\n",
    "\n",
    "            # If sub-menu is found, find all anchor tags within the sub-menu and extract href attributes\n",
    "            if sub_menu:\n",
    "                links = [urljoin(url, a['href']) for a in sub_menu.find_all('a', href=True) if not a['href'].startswith(('javascript:', 'tel:'))]\n",
    "\n",
    "        # Loop through the links\n",
    "        for link in links:\n",
    "            scrape_and_save(link, directory, depth - 1)  # Recursively scrape and save content with reduced depth\n",
    "\n",
    "    else:\n",
    "        print(\"Failed to retrieve the webpage. Status code:\", response.status_code)\n",
    "\n",
    "\n",
    "# URL of the page you want to scrape\n",
    "base_url = \"https://help.relativity.com/RelativityOne/Content/index.htm\"\n",
    "\n",
    "# Create a new directory for the results\n",
    "os.makedirs('results', exist_ok=True)\n",
    "\n",
    "# Scrape and save content with depth limited to two levels\n",
    "scrape_and_save(base_url, depth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same as above, but start on the Capabilities page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_and_save(url, directory='results_pidetect', depth=1):\n",
    "    # Send a GET request to the webpage\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # If the GET request is successful, the status code will be 200\n",
    "    if response.status_code == 200:\n",
    "        # Get the content of the response\n",
    "        content = response.content\n",
    "\n",
    "        # Create a BeautifulSoup object and specify the parser\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "        # Find the main content\n",
    "        main_content = soup.find('div', id=\"mc-main-content\")\n",
    "\n",
    "        # Save the main content to a file\n",
    "        if main_content:\n",
    "            with open(os.path.join(directory, f'{urlparse(url).path.split(\"/\")[-1]}.txt'), 'w', encoding='utf-8') as f:\n",
    "                f.write(main_content.get_text())\n",
    "\n",
    "        # Initialize links as an empty list\n",
    "        links = []\n",
    "\n",
    "        # If depth is 1, find normal href links\n",
    "        if depth == 2:\n",
    "            links = [urljoin(url, a['href']) for a in soup.find_all('a', href=True) if not a['href'].startswith(('javascript:', 'tel:'))]\n",
    "        # If depth is greater than 1, look for the sub-menu\n",
    "        elif depth > 0:\n",
    "            # Find the sub-menu element\n",
    "            sub_menu = soup.find(class_='guideTOC')\n",
    "\n",
    "            # If sub-menu is found, find all anchor tags within the sub-menu and extract href attributes\n",
    "            if sub_menu:\n",
    "                links = [urljoin(url, a['href']) for a in sub_menu.find_all('a', href=True) if not a['href'].startswith(('javascript:', 'tel:'))]\n",
    "\n",
    "        # Loop through the links\n",
    "        for link in links:\n",
    "            scrape_and_save(link, directory, depth - 1)  # Recursively scrape and save content with reduced depth\n",
    "\n",
    "    else:\n",
    "        print(\"Failed to retrieve the webpage. Status code:\", response.status_code)\n",
    "\n",
    "\n",
    "# URL of the page you want to scrape\n",
    "base_url = \"https://help.relativity.com/RelativityOne/Content/PI_Detect/PI_Detect.htm\"\n",
    "\n",
    "# Create a new directory for the results\n",
    "os.makedirs('results_pidetect', exist_ok=True)\n",
    "\n",
    "# Scrape and save content with depth limited to two levels\n",
    "scrape_and_save(base_url, depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 45\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to retrieve the webpage. Status code:\u001b[39m\u001b[38;5;124m\"\u001b[39m, response\u001b[38;5;241m.\u001b[39mstatus_code)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Start the scraping process\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m \u001b[43mscrape_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[21], line 28\u001b[0m, in \u001b[0;36mscrape_page\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Loop through the links\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m link \u001b[38;5;129;01min\u001b[39;00m links:\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;66;03m# Recursively scrape the child page\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m     \u001b[43mscrape_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlink\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Find the specified div tags\u001b[39;00m\n\u001b[0;32m     31\u001b[0m paragraphs \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmc-main-content\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[21], line 28\u001b[0m, in \u001b[0;36mscrape_page\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Loop through the links\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m link \u001b[38;5;129;01min\u001b[39;00m links:\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;66;03m# Recursively scrape the child page\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m     \u001b[43mscrape_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlink\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Find the specified div tags\u001b[39;00m\n\u001b[0;32m     31\u001b[0m paragraphs \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmc-main-content\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "    \u001b[1;31m[... skipping similar frames: scrape_page at line 28 (125 times)]\u001b[0m\n",
      "Cell \u001b[1;32mIn[21], line 28\u001b[0m, in \u001b[0;36mscrape_page\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Loop through the links\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m link \u001b[38;5;129;01min\u001b[39;00m links:\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;66;03m# Recursively scrape the child page\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m     \u001b[43mscrape_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlink\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Find the specified div tags\u001b[39;00m\n\u001b[0;32m     31\u001b[0m paragraphs \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmc-main-content\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[21], line 12\u001b[0m, in \u001b[0;36mscrape_page\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscrape_page\u001b[39m(url):\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# Send a GET request to the webpage\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# If the GET request is successful, the status code will be 200\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;66;03m# Get the content of the response\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kwehb\\.virtualenvs\\kw-ds-testing-dC1OrvnL\\Lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kwehb\\.virtualenvs\\kw-ds-testing-dC1OrvnL\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kwehb\\.virtualenvs\\kw-ds-testing-dC1OrvnL\\Lib\\site-packages\\requests\\sessions.py:575\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;66;03m# Create the Request.\u001b[39;00m\n\u001b[0;32m    563\u001b[0m req \u001b[38;5;241m=\u001b[39m Request(\n\u001b[0;32m    564\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod\u001b[38;5;241m.\u001b[39mupper(),\n\u001b[0;32m    565\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    573\u001b[0m     hooks\u001b[38;5;241m=\u001b[39mhooks,\n\u001b[0;32m    574\u001b[0m )\n\u001b[1;32m--> 575\u001b[0m prep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    577\u001b[0m proxies \u001b[38;5;241m=\u001b[39m proxies \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m    579\u001b[0m settings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerge_environment_settings(\n\u001b[0;32m    580\u001b[0m     prep\u001b[38;5;241m.\u001b[39murl, proxies, stream, verify, cert\n\u001b[0;32m    581\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\kwehb\\.virtualenvs\\kw-ds-testing-dC1OrvnL\\Lib\\site-packages\\requests\\sessions.py:483\u001b[0m, in \u001b[0;36mSession.prepare_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    481\u001b[0m auth \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mauth\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrust_env \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m auth \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauth:\n\u001b[1;32m--> 483\u001b[0m     auth \u001b[38;5;241m=\u001b[39m \u001b[43mget_netrc_auth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    485\u001b[0m p \u001b[38;5;241m=\u001b[39m PreparedRequest()\n\u001b[0;32m    486\u001b[0m p\u001b[38;5;241m.\u001b[39mprepare(\n\u001b[0;32m    487\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod\u001b[38;5;241m.\u001b[39mupper(),\n\u001b[0;32m    488\u001b[0m     url\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39murl,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    498\u001b[0m     hooks\u001b[38;5;241m=\u001b[39mmerge_hooks(request\u001b[38;5;241m.\u001b[39mhooks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhooks),\n\u001b[0;32m    499\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\kwehb\\.virtualenvs\\kw-ds-testing-dC1OrvnL\\Lib\\site-packages\\requests\\utils.py:222\u001b[0m, in \u001b[0;36mget_netrc_auth\u001b[1;34m(url, raise_errors)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# os.path.expanduser can fail when $HOME is undefined and\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# getpwuid fails. See https://bugs.python.org/issue20164 &\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# https://github.com/psf/requests/issues/1846\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    223\u001b[0m     netrc_path \u001b[38;5;241m=\u001b[39m loc\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# URL of the page you want to scrape\n",
    "base_url = \"https://help.relativity.com/RelativityOne/Content/Relativity/aiR_for_Review/aiR_for_Review.htm\"\n",
    "\n",
    "# Create a new directory for the results\n",
    "os.makedirs('results', exist_ok=True)\n",
    "\n",
    "# Send a GET request to the webpage\n",
    "response = requests.get(base_url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    content = response.content\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    paragraphs = soup.find_all('div', id=\"mc-main-content\")\n",
    "    with open(os.path.join('results', 'aiR_for_Review.htm.txt'), 'w', encoding='utf-8') as f:\n",
    "        # Loop through the paragraphs, get the text of each one, and write it to the file\n",
    "        for paragraph in paragraphs:\n",
    "            f.write(paragraph.get_text())\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage. Status code:\", response.status_code)\n",
    "\n",
    "# If the GET request is successful, the status code will be 200\n",
    "if response.status_code == 200:\n",
    "    \n",
    "    # Get the content of the response\n",
    "    content = response.content\n",
    "\n",
    "    # Create a BeautifulSoup object and specify the parser\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "    # Find the sub-menu element\n",
    "    sub_menu = soup.find(class_='guideTOC')\n",
    "\n",
    "    # Find all anchor tags within the sub-menu and extract href attributes\n",
    "    links = [urljoin(base_url, a['href']) for a in sub_menu.find_all('a', href=True)]\n",
    "\n",
    "    # Loop through the links\n",
    "    for link in links:\n",
    "        # Send a GET request to the child page\n",
    "        response = requests.get(link)\n",
    "\n",
    "        # If the GET request is successful, the status code will be 200\n",
    "        if response.status_code == 200:\n",
    "            # Get the content of the response\n",
    "            content = response.content\n",
    "\n",
    "            # Create a BeautifulSoup object and specify the parser\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "            # Find the specified div tags\n",
    "            paragraphs = soup.find_all('div', id=\"mc-main-content\")\n",
    "\n",
    "            # Get the page name from the link\n",
    "            page_name = urlparse(link).path.split('/')[-1]\n",
    "\n",
    "            # Open a file with the page name in the results directory\n",
    "            with open(os.path.join('results', f'{page_name}.txt'), 'w', encoding='utf-8') as f:\n",
    "                # Loop through the paragraphs, get the text of each one, and write it to the file\n",
    "                for paragraph in paragraphs:\n",
    "                    f.write(paragraph.get_text())\n",
    "        else:\n",
    "            print(\"Failed to retrieve the webpage. Status code:\", response.status_code)\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage. Status code:\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "aiR for Review\n",
      "aiR for Review harnesses the power of large language models, or LLMs, to review documents. aiR for Review goes far beyond existing classifiers by using generative AI to both predict coding decisions and to support those predictions with descriptive text and document excerpts which explain the decisions.\n",
      "Some benefits of aiR for Review include:\n",
      "\n",
      "\n",
      "Highly efficient, low-cost document analysis\n",
      "\n",
      "\n",
      "Quick discovery of important issues and criteria\n",
      "\n",
      "\n",
      "Consistent, cohesive analysis across all documents\n",
      "\n",
      "\n",
      "Note: aiR for Review is currently in limited release. For information about the general release, contact your account representative.\n",
      "See these related pages:\n",
      "\n",
      "Running aiR for Review\n",
      "\n",
      "Monitoring aiR for Review jobs\n",
      "\n",
      "aiR for Review results\n",
      "\n",
      "aiR for Review security permissions\n",
      "\n",
      "\n",
      "aiR for Review overview\n",
      "aiR for Review uses generative AI to simulate the actions of a human reviewer, finding and describing relevant documents according to the review instructions that you provide. It identifies the documents, describes why they are relevant using natural language, and demonstrates relevance using citations from the document.\n",
      "aiR for Review has three different analysis types:\n",
      "\n",
      "\n",
      "Relevance review—predict documents responsive to a request for production.\n",
      "\n",
      "\n",
      "Issues review—locate material relating to different legal issues.\n",
      "\n",
      "\n",
      "Key documents—find hot documents important to a case or investigation.\n",
      "\n",
      "\n",
      "Some use cases for aiR for Review include:\n",
      "\n",
      "\n",
      "Kickstarting the review process—prioritize the most important documents to give to reviewers.\n",
      "\n",
      "\n",
      "First-pass review—determine what you need to produce and discover essential insights.\n",
      "\n",
      "\n",
      "Gaining early case insights—learn more about your matter right from the start.\n",
      "\n",
      "\n",
      "Internal investigations—find documents and insights that help you understand the story hidden in your data.\n",
      "\n",
      "\n",
      "Analyzing productions from other parties—reduce the effort to find important material and get it into the hands of decision makers.\n",
      "\n",
      "\n",
      "Quality control for traditional review—compare aiR for Review's coding predictions to decisions made by reviewers to accelerate QC and improve results.\n",
      "\n",
      "\n",
      "aiR for Review workflow\n",
      "aiR for Review's process is similar to training a human reviewer: explain the case and its relevance criteria, hand over the documents, and check the results. If aiR misunderstood any part of the relevance criteria, explain that part in more detail, then try again.\n",
      "Within Relativity, the main steps are:\n",
      "\n",
      "\n",
      "Select the documents to review\n",
      "\n",
      "\n",
      "Choose the aiR for Review mass action\n",
      "\n",
      "\n",
      "Write and submit the review instructions, called Prompt Criteria\n",
      "\n",
      "\n",
      "Review the results\n",
      "\n",
      "\n",
      "When setting up the first analysis, we recommend running it on a sample set of documents that was already coded by human reviewers. If aiR's predictions are different from the human coding, revise the Prompt Criteria and try again. This could include rewriting unclear instructions, defining an acronym or a code word, or adding more detail to an issue definition.\n",
      "Overall, the workflow has three phases:\n",
      "\n",
      "\n",
      "Develop—write the Prompt Criteria, test, and tweak until the results match human review.\n",
      "\n",
      "\n",
      "Verify—run the Prompt Criteria on a slightly larger set of documents and compare to results from senior reviewers.\n",
      "\n",
      "\n",
      "Run—use the verified Prompt Criteria on a much larger set of documents.\n",
      "\n",
      "\n",
      "For more details, see Running aiR for Review. For additional workflow help and examples, see Workflows for Applying aiR for Review on the Community site.\n",
      "How aiR for Review works\n",
      "aiR for Review's analysis is powered by Azure OpenAI's GPT-4 large language model, or LLM. The LLM is designed to understand and generate human language, and it is trained on billions of documents from open datasets and the web.\n",
      "When you submit Prompt Criteria and a set of documents to aiR for Review, Relativity sends the first document to Azure OpenAI and asks it to review the document according to the Prompt Criteria. After Azure OpenAI returns its results, Relativity sends the next document. The LLM reviews each document independently, and it does not learn from previous documents. Unlike Review Center, which makes its predictions based on learning from the document set, the LLM makes its predictions based on the Prompt Criteria and its preexisting training.\n",
      "Azure OpenAI does not retain any data from the documents being analyzed. Data you submit for processing by Azure OpenAI is not retained beyond your organization’s instance, nor is it used to train any other generative AI models from Relativity, Microsoft, or any other third party. For more information, see the white paper A Focus on Security and Privacy in Relativity’s Approach to Generative AI.\n",
      "For more information on using generative AI for document review, we recommend:\n",
      "\n",
      "\n",
      "Relativity Webinar - AI Advantage: How to Accelerate Review with Generative AI\n",
      "\n",
      "\n",
      "\n",
      "MIT's Generative AI for Law resources\n",
      "\n",
      "\n",
      "\n",
      "The State Bar of California's drafted recommendations for the use of generative AI\n",
      "\n",
      "\n",
      "\n",
      "Using aiR for Review with non-English languages\n",
      "aiR for Review currently supports English-language Prompt Criteria and analyzing English text. It has not been tested for use with other languages, and doing so may result in unexpected or unverifiable results. However, Relativity will continue to do additional testing and analysis on non-English language scenarios.\n",
      "Using aiR for Review with emojis\n",
      "aiR for Review has not been specifically tested for analyzing emojis. However, the underlying LLM does understand Unicode emojis. It also understands other formats that could normally be understood by a human reviewer. For example, an emoji that is extracted to text as :smile: would be understood as smiling.\n",
      "Archiving and restoring workspaces with aiR for Review\n",
      "Workspaces with aiR for Review installed can be archived and restored using the ARM application.\n",
      "When archiving, check Include Extended Workspace Data under Extended Workspace Data Options. If this option was not checked during the archive process, the aiR for Review features in the restored workspace will not be fully functional.  If this happens, you will need to manually reinstall aiR for Review in the restored workspace.\n",
      "For more information on using ARM, see ARM.\n",
      " \n",
      "\n",
      "Running aiR for Review\n",
      "aiR for Review uses generative AI to simulate the actions of a human reviewer, finding and describing relevant documents using the review instructions that you provide. These analyses can be customized to search for relevance, key documents, or specific issues as needed.\n",
      "The instructions you give aiR for Review are called Prompt Criteria. For best results, we recommend analyzing a small set of documents, tweaking the Prompt Criteria as needed, then finally analyzing a larger set of documents. This lets you see immediately how aiR's coding compares to a human reviewer's coding and adjust the prompts accordingly.\n",
      "Note: aiR for Review is currently in limited release. For information about the general release, contact your account representative.\n",
      "See these related pages:\n",
      "\n",
      "aiR for Review\n",
      "\n",
      "Monitoring aiR for Review jobs\n",
      "\n",
      "aiR for Review results\n",
      "\n",
      "aiR for Review security permissions\n",
      "\n",
      "\n",
      "See these additional resources:\n",
      "\n",
      "Workflows for Applying aiR for Review\n",
      "\n",
      "aiR for Review example project\n",
      "\n",
      "\n",
      "Installing aiR for Review\n",
      "aiR for Review is available as a secured application from the Application Library. You must have an active aiR for Review contract to use it, and it is not available for repository workspaces.\n",
      "To install it:\n",
      "\n",
      "\n",
      "Navigate to the Relativity Applications tab in your workspace.\n",
      "\n",
      "\n",
      "Select Install from application library.\n",
      "\n",
      "\n",
      "Select the aiR for Review application.\n",
      "\n",
      "\n",
      "Click Install.\n",
      "\n",
      "\n",
      "After installation completes, the following object types will appear in your workspace:\n",
      "\n",
      "\n",
      "aiR Relevance Analysis—records the Relevance results of aiR for Review analysis runs.\n",
      "\n",
      "\n",
      "aiR Issue Analysis—records the Issue results of aiR for Review analysis runs.\n",
      "\n",
      "\n",
      "aiR Key Analysis—records the Key results of aiR for Review analysis runs.\n",
      "\n",
      "\n",
      "aiR for Review Prompt Criteria—records the Prompt Criteria settings and contents for each analysis run. This also records Prompt Criteria drafts for each user.\n",
      "\n",
      "\n",
      "Installing aiR for Review creates two versions of the aiR for Review Jobs tab: one at the instance level, and one at the workspace level. Some users also find it helpful to manually create tabs for the three Analysis objects, but this is optional.\n",
      "For more information on installing applications, see Relativity applications.\n",
      "Setting up permissions\n",
      "For detailed information on aiR for Review user permissions, see aiR for Review security permissions.\n",
      "Choosing an analysis type\n",
      "aiR for Review supports three types of analysis. Each one is geared towards a different phase of a review or investigation.\n",
      " For each aiR for Review job, choose one analysis type:\n",
      "\n",
      "\n",
      "Relevance—analyzes whether documents are relevant to a case or situation that you describe, such as documents responsive to a production request.\n",
      "\n",
      "\n",
      "Relevance and Key Documents—analyzes documents for both relevance and whether they are “hot” or key to a case.\n",
      "\n",
      "\n",
      "Issues—analyzes documents for whether they include content that falls under specific categories. For example, you might use this to check whether documents involve coercion, retaliation, or a combination of both.\n",
      "\n",
      "\n",
      "Based on the analysis type you choose, you will need the following fields:\n",
      "\n",
      "\n",
      "Relevance—one single-choice results field. The field must have at least one choice.\n",
      "\n",
      "\n",
      "Relevance and Key Documents—two single-choice results fields. These should have distinct names such as \"Relevant\" and \"Key,\" and each field should have at least one choice.\n",
      "\n",
      "\n",
      "Issues—one multi-choice results field. Each choice should represent one of the issues you want to analyze.\n",
      "Note: Currently, aiR for Review analyzes a maximum of five issues per run. You can have as many choices for the field as you want, but you can only analyze five at a time.\n",
      "\n",
      "\n",
      "aiR for Review does not actually write to these fields. Instead, it uses them as reference when making and reporting on its predictions.\n",
      "Choosing a processing mode\n",
      "aiR for Review supports two processing modes: Fast Track and Batch. These are designed for the workflow of fine-tuning Prompt Criteria on a small set of documents, then using the Prompt Criteria on a large set of documents.\n",
      "Because the large language model (LLM) has limited capacity, we dedicate some bandwidth exclusively to Fast Track jobs. This returns speedy results for smaller jobs without them having to wait for larger jobs in the queue.\n",
      " For each aiR for Review job, choose one mode:\n",
      "\n",
      "\n",
      "Fast Track—processes up to 50 documents quickly. Use this to test and refine your Prompt Criteria on a small set of test documents.\n",
      "\n",
      "\n",
      "Each user can have only one Fast Track job running at a time.\n",
      "\n",
      "\n",
      "These jobs use dedicated bandwidth that is not available to larger batch jobs. They typically return results within 5-10 minutes.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batch—processes up to 50,000 documents. Use this to run your previously refined Prompt Criteria on a larger set of documents.\n",
      "\n",
      "\n",
      "Each instance can have up to three Batch jobs running at a time.\n",
      "\n",
      "\n",
      "Processing time varies based on document load and total load on the LLM.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "For more detailed information about each mode’s capacity, see Job capacity and size limitations.\n",
      "Best practices for running aiR for Review\n",
      "aiR for Review works best after fine-tuning the Prompt Criteria. Analyzing just a few documents at first, comparing the results to human coding, and then adjusting the Prompt Criteria as needed yields more accurate results than diving in with a full document set.\n",
      "We recommend the following workflow:\n",
      "\n",
      "\n",
      "For your first analysis, run the Prompt Criteria on a set of 10 test documents that are a mix of relevant and  not relevant.\n",
      "\n",
      "\n",
      "Compare the results to human coding. In particular, look for documents that aiR coded differently than the humans did and investigate possible reasons. This could include unclear instructions, needing to define an acronym or code word, or other blind spots in the Prompt Criteria.\n",
      "\n",
      "\n",
      "Tweak the Prompt Criteria to adjust for blind spots.\n",
      "\n",
      "\n",
      "Repeat steps 1 through 3 until aiR predicts coding decisions accurately for the test documents.\n",
      "\n",
      "\n",
      "Test the Prompt Criteria on 50 documents and compare results. Continue tweaking as needed.\n",
      "\n",
      "\n",
      "Finally, run the Prompt Criteria on a larger set of documents.\n",
      "\n",
      "\n",
      "aiR only sees the extracted text of a document. It does not see any non-text elements like advanced formatting, embedded images, or videos. We do not recommend using aiR for Review on documents such as images, videos, or spreadsheets with heavy formulas. Instead, use it on documents whose extracted text accurately represents their content and meaning.\n",
      "Running the analysis\n",
      "aiR for Review works as a mass action found on the Documents tab. Running the analysis has three basic parts:\n",
      "\n",
      "\n",
      "Selecting documents and setting up the review\n",
      "\n",
      "\n",
      "Writing the Prompt Criteria\n",
      "\n",
      "\n",
      "Submitting the job for analysis\n",
      "\n",
      "\n",
      "At any point in this process, you can click Save and Close in the mass action modal. This saves your progress so that you can keep working on it at a later time.\n",
      "When you reopen the mass action modal, the last Prompt Criteria that you saved will display. For more information, see Running aiR for Review.\n",
      "Step 1: Selecting documents and setting up the review\n",
      "To start an aiR for Review analysis job:\n",
      "\n",
      "\n",
      "From the Documents tab, select the documents you want to analyze.\n",
      "\n",
      "\n",
      "Under Mass Actions, select aiR for Review. A modal with several tabs appears.\n",
      "\n",
      "\n",
      "On the Setup tab of the modal, set the following:\n",
      "\n",
      "\n",
      "Prompt Criteria Name—give the Prompt Criteria a unique name. You can also click Load Prompt Criteria to select Prompt Criteria that you or another user previously wrote. For more information, see Editing and collaboration.\n",
      "\n",
      "\n",
      "Review Type—select one of the following. For more information, see Choosing an analysis type.\n",
      "\n",
      "Relevance—analyzes whether documents are relevant to a case or situation that you describe, such as documents responsive to a production request.\n",
      "\n",
      "Relevance and Key Documents—analyzes documents for both relevance and whether they are “hot” or key to a case.\n",
      "\n",
      "\n",
      "Issues—analyzes documents for whether they include content that falls under specific categories.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Run in Fast Track—toggle this On for 50 documents or fewer, and Off for more than 50 documents. For more information, see Choosing a processing mode.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The first time you use the mass action, all the fields will be blank. When you click Save and Next or Save and Close, your progress saves and persists for the next analysis.\n",
      "If any required fields on any of the tabs are empty or misconfigured, the Save and Next button will be unavailable. Click on the title of each tab to fill out its fields.\n",
      "Step 2: Writing the Prompt Criteria\n",
      "The Prompt Criteria are a set of inputs that give aiR the context it needs to understand the matter and evaluate each document. Writing the Prompt Criteria is a way of training your \"reviewer,\" similar to training a human reviewer.\n",
      "Depending which type of analysis you chose, you will see a different set of tabs. All Prompt Criteria include the Case Summary tab.\n",
      "General writing guidelines\n",
      "For all of the setup tabs, we recommend:\n",
      "\n",
      "\n",
      "Write as if \"less is more.\" Instead of pasting in a long review protocol as-is, summarize where possible and include only key passages. The Prompt Criteria have an overall length limit of 10,000 characters.\n",
      "\n",
      "\n",
      "Phrase things in a positive way when possible. Avoid negatives (\"not\" statements) and double negatives.\n",
      "\n",
      "\n",
      "Do not include explanations of the law.\n",
      "\n",
      "\n",
      "Do not give the LLM commands, such as “you will review XX.\" Instead, simply describe the case.\n",
      "\n",
      "\n",
      "Use whatever writing format makes the most sense to a human reader. For example, bullet points might be useful for the People and Aliases section, but paragraphs might make sense in another section.\n",
      "\n",
      "\n",
      "The LLM has essentially “read the whole Internet.” It understands widely used slang and abbreviations, but it does not necessarily know jargon or phrases that are specific to an organization.\n",
      "\n",
      "\n",
      "When you start to write your first Prompt Criteria, the fields contain grayed-out helper text that shows examples of what to enter. Use this as a guideline for crafting your own entries.\n",
      "Note: \n",
      "For more guidance on prompt writing, see the following resources on the Community site:\n",
      "\n",
      "\n",
      "aiR for Review Prompt Writing Best Practices—downloadable PDF of writing guidelines\n",
      "\n",
      "\n",
      "aiR for Review example project—detailed example of adapting a review protocol into Prompt Criteria\n",
      "\n",
      "\n",
      "\n",
      "Filling out the Case Summary tab\n",
      "The Case Summary gives the broad context surrounding a matter. It includes an overview of the matter, people and entities involved, and any jargon or terms that are needed to understand the document set.\n",
      "Limit the Case Summary content to roughly 20 sentences overall, and 20 each of People and Aliases, Noteworthy Organizations, and Noteworthy Terms.\n",
      "To fill out the Case Summary tab:\n",
      "\n",
      "\n",
      "Within the setup modal, click on the Case Summary tab.\n",
      "\n",
      "\n",
      "Fill out the following:\n",
      "\n",
      "\n",
      "Matter Overview—provide a concise overview of the case. Include the names of the plaintiff and defendant, the nature of the dispute, and other important case characteristics.\n",
      "\n",
      "\n",
      "People and Aliases—list the names and aliases of key custodians who authored or received the documents. Include their role and any other affiliations.\n",
      "\n",
      "\n",
      "Noteworthy Organizations—list the organizations and other relevant entities involved in the case. Highlight any key relationships or other notable characteristics.\n",
      "\n",
      "\n",
      "Noteworthy Terms—list and define any relevant words, phrases, acronyms, jargon, or slang that might be important to the analysis.\n",
      "\n",
      "\n",
      "Additional Context—list any additional information that does not fit the other fields. This section is typically left blank.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Depending on which Review Type you chose, the remaining tabs will be called Relevance, Key Documents, or Issues. Fill out those tabs according to the guide sections below.\n",
      "Filling out the Relevance tab\n",
      "If you chose either Relevance or Relevance and Key Documents as the Review Type, you will see the Relevance tab. This defines the fields and criteria used for determining if a document is relevant to the case.\n",
      "To fill out the Relevance tab:\n",
      "\n",
      "\n",
      "Within the setup modal, click on the Relevance tab.\n",
      "\n",
      "\n",
      "Fill out the following:\n",
      "\n",
      "\n",
      "Relevance Field—select a single-choice field that represents whether a document is relevant or non-relevant.\n",
      "\n",
      "\n",
      "Relevant Choice—select the field choice you use to mark a document as relevant.\n",
      "\n",
      "\n",
      "Relevance Criteria—summarize the criteria that determine whether a document is relevant. Include:\n",
      "\n",
      "\n",
      "Keywords, phrases, legal concepts, parties, entities, and legal claims\n",
      "\n",
      "\n",
      "Any criteria that would make a document non-relevant, such as relating to a project that is not under dispute\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Issues Field (Optional)—select a single-choice or multi-choice field that represents the issues in the case.\n",
      "\n",
      "\n",
      "Choice Criteria—select each of the field choices one by one. For each choice, write a summary in the text box listing the criteria that determine whether that issue applies to a document. For more information, see Filling out the Issues tab.\n",
      "\n",
      "\n",
      "Note: aiR does not make Issue predictions during Relevance review, but you can use this field for reference when writing the Relevance Criteria. For example, you could tell aiR that any documents related to these issues are relevant.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "For best results when writing the Relevance Criteria:\n",
      "\n",
      "\n",
      "Limit the Relevance Criteria to 5-10 sentences.\n",
      "\n",
      "\n",
      "Do not paste in the original request for production (RFP); those are often too long and complex to give good results. Instead, summarize it and include key excerpts.\n",
      "\n",
      "\n",
      "Group similar criteria together when you can. For example, if an RFP asks for “emails pertaining to X” and “documents pertaining to X,” write “emails or documents pertaining to X.”\n",
      "\n",
      "\n",
      "Filling out the Key Documents tab\n",
      "If you chose Relevance and Key as the Review Type, you will see the Key Documents tab. This defines the fields and criteria used for determining if a document is \"hot\" or key to the case.\n",
      "To fill out the Key Documents tab:\n",
      "\n",
      "\n",
      "Within the setup modal, click on the Key Documents tab.\n",
      "\n",
      "\n",
      "Fill out the following:\n",
      "\n",
      "\n",
      "Key Document Field—select a single-choice field that represents whether a document is key to the case.\n",
      "\n",
      "\n",
      "Key Document Choice—select the field choice you use to mark a document as key.\n",
      "\n",
      "\n",
      "Key Document Criteria—summarize the criteria that determine whether a document is key. Include:\n",
      "\n",
      "\n",
      "Keywords, phrases, legal concepts, parties, entities, and legal claims\n",
      "\n",
      "\n",
      "Any criteria that would exclude a document from being key, such as falling outside a certain date range\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "For best results, limit the Key Document Criteria to 5-10 sentences.\n",
      "Filling out the Issues tab\n",
      "If you chose Issues as the Review Type, you will see the Issues tab. This defines the fields and criteria used for determining whether a document relates to a set of specific topics or issues.\n",
      "To fill out the Issues tab:\n",
      "\n",
      "\n",
      "Within the setup modal, click on the Issues tab.\n",
      "\n",
      "\n",
      "Fill out the following:\n",
      "\n",
      "\n",
      "Field—select a multi-choice field that represents the issues in the case.\n",
      "\n",
      "\n",
      "Choice Criteria—select each of the field choices one by one. For each choice, write a summary in the text box listing the criteria that determine whether that issue applies to a document. Include:\n",
      "\n",
      "\n",
      "Keywords, phrases, legal concepts, parties, entities, and legal claims\n",
      "\n",
      "\n",
      "Any criteria that would exclude a document from being key, such as falling outside a certain date range\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "For best results when writing the Choice Criteria:\n",
      "\n",
      "\n",
      "Limit the criteria description for each choice to 5-10 sentences.\n",
      "\n",
      "\n",
      "Each of the choices must have its own criteria. If a choice has no criteria, either fill it in or remove the choice.\n",
      "\n",
      "\n",
      "Removing issue choices\n",
      "aiR analyzes a maximum of 5 choices. If the issue field has more than 5 choices:\n",
      "\n",
      "\n",
      "Select the choice you want to remove.\n",
      "\n",
      "\n",
      "Click the Remove Choice button on the right.\n",
      "\n",
      "\n",
      "Repeat with any other unwanted choices.\n",
      "\n",
      "\n",
      "Step 3: Submitting the job for analysis\n",
      "After filling out the Setup, Case Summary, and other tabs, review the job and submit it for analysis.\n",
      "To submit a job:\n",
      "\n",
      "\n",
      "Click Save and Next.\n",
      "\n",
      "\n",
      "Review the confirmation summary. This includes:\n",
      "\n",
      "\n",
      "Total Docs—number of documents to be analyzed.\n",
      "\n",
      "\n",
      "Est. Total Doc Units—number of document units counted for billing purposes. For more information, see How document units are calculated.\n",
      "\n",
      "\n",
      "Est. Time to Start—estimated wait time from when you submit the job, to when aiR can begin analyzing your job. Longer wait times appear when aiR already has other work queued up.\n",
      "\n",
      "\n",
      "Est. Run Time—estimated time aiR will take to analyze and return the results of the documents selected. This does not include time waiting in the queue.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Click Start Classification.\n",
      "A banner appears showing that the job was successfully submitted for analysis.\n",
      "\n",
      "\n",
      "Note: If you try to run a job that is too large or when too many jobs are already running, an error will appear. You can still save and edit the Prompt Criteria, but you will not be able to start the job. For more information, see Job capacity and size limitations.\n",
      "For information on monitoring jobs in progress, see Monitoring aiR for Review jobs.\n",
      "Editing and collaboration\n",
      "By default, when you select the mass action, the last Prompt Criteria you saved will display. This makes it easy to edit the Prompt Criteria without re-entering information.\n",
      "If you want to edit a different set of Prompt Criteria or collaborate with another user, you can load previous Prompt Criteria into the mass action modal. From there, any edits you make will be saved as a new set of Prompt Criteria.\n",
      "To load previous Prompt Criteria:\n",
      "\n",
      "\n",
      "From the Documents tab, check the documents you want to analyze.\n",
      "\n",
      "\n",
      "Under Mass Actions, select aiR for Review.\n",
      "\n",
      "\n",
      "On the Setup tab of the modal,  click Load Prompt Criteria. A pop-up opens with two tabs:\n",
      "\n",
      "\n",
      "Prompt Criteria—Prompt Criteria for jobs that already ran in the workspace.\n",
      "\n",
      "\n",
      "Drafts—each user’s most recently saved Prompt Criteria. These may or may not have been run yet.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Select a row from either tab.\n",
      "The right-hand panel shows a preview of the Prompt Criteria.\n",
      "\n",
      "\n",
      "Click Load.\n",
      "The Prompt Criteria Name, Analysis Type, and all criteria load into the aiR for Review modal.\n",
      "\n",
      "\n",
      "If you run the previous Prompt Criteria without making any changes, the Prompt Criteria Name stays the same. If you edit the prompt before running it, a number such as (1) or (2) will be automatically added to the end of the Prompt Criteria Name. You can also manually enter your own name for the Prompt Criteria as you edit it.\n",
      "If you and another user both edit the same Prompt Criteria at the same time, your edits are saved as separate drafts. To collaborate on the same draft, we recommend having the first user finish their edits, then pass the draft off to the second user.\n",
      "Note: Only one Prompt Criteria draft is saved for each user. If you save a draft, then load in different Prompt Criteria, that draft will be overwritten. To save Prompt Criteria long-term, run them with one or more documents.\n",
      "How document units are calculated\n",
      "A document unit is a document with between 1 and 15,000 characters of text. If a document has more than 15,000 characters in it, it is counted as two or more document units. For example, a 1-character document counts as one document unit, but a 16,000-character document counts as two document units.\n",
      "Note: Any Unicode character counts as one character, regardless of storage size.\n",
      "Because the document unit estimation calculates white space slightly differently than actual billing, small discrepancies may appear for document sizes that are right on the border between two document units. To find the actual document units that are billed, see the Cost Explorer.\n",
      "Job capacity and size limitations\n",
      "Based on the limits of the underlying large language model (LLM), aiR has size limits for the documents and prompts you submit, as well as volume limits for the overall jobs.\n",
      "Size limits\n",
      "The documents and Prompt Criteria have the following size limits:\n",
      "\n",
      "\n",
      "The Prompt Criteria have an overall length limit of 10,000 characters.\n",
      "\n",
      "\n",
      "Each document's extracted text should be under 150KB if possible. aiR has a hard limit of 300KB extracted text per document, but due to how the LLM processes document size, it sometimes rejects documents that are smaller than 300KB. To avoid this, we recommend treating 150KB as the upper limit.\n",
      "\n",
      "\n",
      "Each document's extracted text, when combined with the Prompt Criteria, must be less than 32,000 “tokens” (roughly equivalent to words, symbols, or whitespace). This is usually not a problem for documents under 150KB.\n",
      "\n",
      "\n",
      "Volume limits\n",
      "The volume limits for aiR for Review jobs are as follows:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Volume Type\n",
      "Limit\n",
      "Notes\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Max job size for Fast Track mode\n",
      "50 documents\n",
      "For over 50 documents, use Batch mode.\n",
      "\n",
      "\n",
      " Max job size for Batch mode \n",
      "50,000 documents\n",
      "\n",
      "A single Batch job can include up to 50,000 documents.\n",
      "\n",
      "\n",
      "\n",
      "Concurrent Fast Track jobs per user\n",
      "1 job\n",
      "Each user can only have one Fast Track job running at a time. However, multiple users in one instance can run Fast Track jobs at the same time.\n",
      "\n",
      "\n",
      "Concurrent Batch jobs per instance\n",
      "3 jobs\n",
      "Only 3 Batch jobs can be queued or running at the same time within an instance.\n",
      "\n",
      "\n",
      "\n",
      "Speed\n",
      "After a job is submitted, aiR analyzes roughly 25-50 documents per minute. Fast Track jobs typically take under 10 minutes. Batch job speeds vary widely depending on the number of documents, the overall load on the LLM, and other factors.\n",
      "\n",
      "Monitoring aiR for Review jobs\n",
      "After an aiR for Review job has started, you can use the aiR for Review jobs tab to monitor its progress, view prompt details, or cancel it. You can also view completed jobs and choose which analysis results are connected to the documents.\n",
      "For Fast Track jobs, banner statuses appear on the Documents tab to let you know when a job is complete. You can also monitor these from the aiR for Review jobs tab like any other job.\n",
      "Note: aiR for Review is currently in limited release. For information about the general release, contact your account representative.\n",
      "See these related pages:\n",
      "\n",
      "aiR for Review\n",
      "\n",
      "Running aiR for Review\n",
      "\n",
      "aiR for Review results\n",
      "\n",
      "aiR for Review security permissions\n",
      "\n",
      "\n",
      "aiR for Review Jobs tab\n",
      "There are two versions of the aiR for Review Jobs tab: one at the instance level, and one at the workspace level. The instance-level tab shows all jobs across all workspaces, and it includes several extra columns to identify the workspace, matter, and client connected to each job. Most users only need access to the workspace-level tab. However, because some of aiR's volume limits are instance-wide, the instance-level tab makes it easy to see exactly how much capacity is being used.\n",
      "Both versions of the tab show aiR for Review jobs that have been submitted for analysis. You can use the tab to view prompt details, cancel queued or in-progress jobs, and manage the job results.\n",
      "For information on managing tab permissions, see aiR for Review security permissions.\n",
      "Note: If the aiR for Review Jobs tab says that aiR for Review is not currently available, check with your administrator. Your organization might not have an active contract for aiR for Review.\n",
      "Managing jobs and document linking\n",
      "You can use the aiR for Review Jobs tab to cancel jobs, clear job results out of the document fields, and restore previous job results.\n",
      "To manage jobs, use the following icons:\n",
      "\n",
      "\n",
      "Cancel symbol (\n",
      ")—cancels a queued or in-progress job. Any results that were already received from the large language model (LLM) will stay in the fields, and those results will still be billed.\n",
      "\n",
      "\n",
      "Clear symbol (\n",
      ")—clears job results from the documents in this run. This empties the aiR for Review fields and removes highlighting from the Viewer, but it does not permanently delete the results. The results can be restored and re-linked at any time.\n",
      "\n",
      "\n",
      "Restore symbol (\n",
      ")—re-links the results of the selected job to the documents in the run. This replaces the results of any other job with the same result type.\n",
      "\n",
      "\n",
      "For example, if you realize your current Prompt Criteria gives you less helpful results than a previous Prompt Criteria did, you can restore the previous job's results. This immediately gives reviewers access to the old predictions without needing to re-run the old Prompt Criteria.\n",
      "Notes: \n",
      "If you run a new job on documents that were part of a previous job, you may temporarily see both sets of results linked to those documents. The old results will be unlinked after the new job is complete.\n",
      "\n",
      "To avoid seeing doubled results, clear the previous result set using the aiR for Review Jobs tab.\n",
      "\n",
      "\n",
      "Viewing job details\n",
      "To see an aiR for Review job's Prompt Criteria, click on its row. A detail panel opens showing the setup details, case summary, fields, and criteria for analysis.\n",
      "You can control a user's access to the detail panel using both item-level and workspace-level permissions. For more information, see aiR for Review security permissions.\n",
      "Jobs tab fields\n",
      "The following fields appear on the aiR for Review Jobs tab:\n",
      "\n",
      "\n",
      "Job ID—the unique ID assigned to a job.\n",
      "\n",
      "\n",
      "Prompt Criteria Name—the name of the Prompt Criteria used by the job.\n",
      "\n",
      "\n",
      "If several jobs ran using the same Prompt Criteria, this name will be the same for those jobs.\n",
      "\n",
      "\n",
      "If a user edited the Prompt Criteria before running the job but did not change the name, the Prompt Criteria name will have a version number such as (1) or (2) appended after it.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Job Status—the current state of the job. The possible statuses are:\n",
      "\n",
      "\n",
      "Not Started\n",
      "\n",
      "\n",
      "Queued\n",
      "\n",
      "\n",
      "In Progress\n",
      "\n",
      "\n",
      "Completed\n",
      "\n",
      "\n",
      "Cancelling\n",
      "\n",
      "\n",
      "Errored\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Job Type—the job's processing mode. For more information, see Choosing a processing mode.\n",
      "\n",
      "\n",
      "Client Name (workspace-level only)—the client associated with the job's workspace.\n",
      "\n",
      "\n",
      "Matter Name (workspace-level only)—the matter name associated with the job's workspace.\n",
      "\n",
      "\n",
      "Matter Number (workspace-level only)—the matter number associated with the job's workspace.\n",
      "\n",
      "\n",
      "Workspace ID (workspace-level only)—the ID of the job's workspace.\n",
      "\n",
      "\n",
      "Workspace Name (workspace-level only)—the name of the job's workspace.\n",
      "\n",
      "\n",
      "Doc Count—the number of documents submitted for analysis.\n",
      "\n",
      "\n",
      "Docs Successful—the number of documents that were successfully analyzed.\n",
      "\n",
      "\n",
      "Docs Pending—the number of documents that are waiting to be analyzed.\n",
      "\n",
      "\n",
      "Docs Errored—the number of documents that encountered an error during analysis.\n",
      "\n",
      "\n",
      "Docs Skipped—the number of documents that aiR did not return results for. This can happen for reasons such as cancelling a job, network errors, and partial or complete job failures.\n",
      "\n",
      "\n",
      "User Name—the user who submitted the job.\n",
      "\n",
      "\n",
      "Submitted Time—the time the user submitted the job.\n",
      "\n",
      "\n",
      "Completed Time—the time the job successfully completed. If the job failed or was cancelled early, this field is blank.\n",
      "\n",
      "\n",
      "Terminated Time—the time the job stopped running, regardless of whether it was cancelled, failed, or completed successfully.\n",
      "\n",
      "\n",
      "Job Failure Reason—if the job failed, the reason is listed here. If the job completed successfully, this field is blank.\n",
      "\n",
      "\n",
      "Estimated Wait Time—the initial estimate for how long the job will wait between when the user submits the job and when the job can start running.\n",
      "\n",
      "\n",
      "Estimated Run Time—the initial estimate for how long the job will take to run.\n",
      "\n",
      "\n",
      "Document Units—the number of documents counted for billing purposes. For more information, see How document units are calculated.\n",
      "\n",
      "\n",
      "Fast Track banner statuses\n",
      "When you submit a Fast Track job, the confirmation banner on the Documents tab updates automatically to show you the status of the job. This makes it easier to keep track of a small job's status without needing to check the dedicated Jobs tab. This banner only appears for the user who submitted the job.\n",
      "The banner updates when the job is queued, in progress, and complete.\n",
      " \n",
      "\n",
      "aiR for Review results\n",
      "When aiR for Review analyzes documents, it makes predictions about the relevance of documents to different topics or issues. If it predicts that a document is relevant or relates to an issue, it includes a written justification of that prediction, as well as a counterargument and in-text citations. You can view these predictions, citations, and justifications either from the Viewer, or from a custom document view.\n",
      "Note: aiR for Review is currently in limited release. For information about the general release, contact your account representative.\n",
      "See these related pages:\n",
      "\n",
      "aiR for Review\n",
      "\n",
      "Running aiR for Review\n",
      "\n",
      "Monitoring aiR for Review jobs\n",
      "\n",
      "aiR for Review security permissions\n",
      "\n",
      "\n",
      "How aiR for Review results work\n",
      "When aiR for Review finishes its analysis, it returns a prediction about how each document should be categorized, as well as its reasons for that prediction. This analysis has several parts:\n",
      "\n",
      "\n",
      "Rank—a numerical rank that indicates how strongly relevant the document is or how well it matches the predicted issue.\n",
      "\n",
      "\n",
      "Prediction—the relevance, key, or issue label that aiR predicts should apply to the document.\n",
      "\n",
      "\n",
      "Rationale—an explanation of why aiR chose this rank and prediction.\n",
      "\n",
      "\n",
      "Considerations—a counterargument explaining why the rationale might possibly be wrong.\n",
      "\n",
      "\n",
      "Citations—excerpts from the document that support the prediction and rationale.\n",
      "\n",
      "\n",
      "In general, citations are left empty for non-relevant documents and documents that don't match an issue. However, aiR occasionally provides a citation for low-ranking documents if it helps to clarify why it was marked non-relevant. For example, if aiR is searching for changes of venue, it might cite an email that ends with \"Hang on, gotta run, more later\" as worth noting, even though it understands that’s not a true change of venue request.\n",
      "Predictions versus document coding\n",
      "Even though aiR refers to the relevance, key, and issue fields during its analysis, it does not actually write to these fields. All of aiR's results are stored in aiR-specific fields such as the Prediction field. We recommend using these aiR fields for reference and reserving the actual relevance, key, and issue fields for human coding.\n",
      "For ideas on how to integrate aiR for Review results into a larger review workflow, see Using aiR for Review with Review Center.\n",
      "Variability of results\n",
      "Because of how large language models work, results can vary slightly from run to run. aiR's results for an individual document can potentially change even when given the same set of inputs. However, this is relatively rare; from our testing, it happens about 4% of the time.\n",
      "Understanding document ranks\n",
      "aiR ranks documents from 0 to 4 according to how relevant they are or how well they match an issue. The higher the number, the more relevant the document is predicted to be. In addition, aiR assigns a rank of -1 to any errored documents. Because these were not properly analyzed, they cannot receive a normal rank.\n",
      "The aiR for Review ranks are:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Rank\n",
      "Description\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-1\n",
      "The document either encountered an error or could not be analyzed. For more information, see How document errors are handled.\n",
      "\n",
      "\n",
      "0 \n",
      "The document is “junk” data such as system files or sets of random characters.\n",
      "\n",
      "\n",
      "1\n",
      "The document is predicted not relevant. aiR did not find any evidence that it relates to the case or issue.\n",
      "\n",
      "\n",
      "2\n",
      "The document is predicted borderline relevant. aiR found some content that might relate to the case or issue. It usually has citations.\n",
      "\n",
      "\n",
      "3\n",
      "The document is predicted relevant to the issue. Citations show the relevant text.\n",
      "\n",
      "\n",
      "4\n",
      "The document is predicted very relevant to the issue. aiR found direct, strong evidence that the content relates to the case or issue. Citations show the relevant text.\n",
      "\n",
      "\n",
      "\n",
      "Viewing results for individual documents\n",
      "From the Viewer, you can see the aiR for Review results for each individual document. Predictions show up in the left-hand pane, and all citations are automatically highlighted.\n",
      "To view a document's aiR for Review results, click on the aiR for Review Analysis icon () to expand the pane. The aiR for Review Analysis pane displays the following:\n",
      "\n",
      "\n",
      "Analysis Name\n",
      "\n",
      "\n",
      "Prediction\n",
      "\n",
      "\n",
      "Rationale and Considerations\n",
      "\n",
      "\n",
      "Citation\n",
      "\n",
      "\n",
      "Notes: \n",
      "If you run a new job on documents that were part of a previous job, you may temporarily see both sets of results linked to those documents. The old results will be unlinked after the new job is complete.\n",
      "\n",
      "To avoid seeing doubled results, clear the previous result set using the aiR for Review Jobs tab.\n",
      "\n",
      "\n",
      "Citations and highlighting\n",
      "To jump to a specific citation, click the citation card. You can also toggle highlighting on or off by clicking the toggle at the top of the aiR for Review Analysis pane.\n",
      "The highlight colors depend on the type of citation:\n",
      "\n",
      "\n",
      "Relevance citation—orange.\n",
      "\n",
      "\n",
      "Key Document citation—purple.\n",
      "\n",
      "\n",
      "Issue citation—color set chosen in the Color Map application. For more information, see Changing the color associated with a coding choice.\n",
      "\n",
      "\n",
      "If the same passage is cited by two types of results, the highlight blends their colors.\n",
      "Adding aiR for Review fields to layouts\n",
      "Because of how aiR for Review results fields are structured, you cannot add them directly to layouts. If the highlighting is not enough, you can add an object list to the layout that shows all linked results. For more information, see Adding and editing an object list.\n",
      "Viewing results for groups of documents\n",
      "You can view and compare aiR for Review results for large groups of documents by adding their fields to document views and saved searches.\n",
      "Each field name is formatted as aiR <review type> Analysis::<fieldname>. For example, the Prediction field for a Relevance analysis is called aiR Relevance Analysis::Prediction.\n",
      "For a full field list, see aiR for Review results fields.\n",
      "Notes: \n",
      "If you run a new job on documents that were part of a previous job, you may temporarily see both sets of results linked to those documents. The old results will be unlinked after the new job is complete.\n",
      "\n",
      "To avoid seeing doubled results, clear the previous result set using the aiR for Review Jobs tab.\n",
      "\n",
      "\n",
      "Creating an aiR for Review results view\n",
      "When creating a view for aiR for Review results, we recommend including these fields:\n",
      "\n",
      "\n",
      "Edit\n",
      "\n",
      "\n",
      "Control Number\n",
      "\n",
      "\n",
      "<Review Field>\n",
      "\n",
      "\n",
      "aiR <Review Type> Analysis::Rank\n",
      "\n",
      "\n",
      "aiR <Review Type> Analysis::Prediction\n",
      "\n",
      "\n",
      "Because the Rationale, Citation, and Considerations fields have larger blocks of text, those tend to be less helpful for comparing many documents. However, you can also add those if desired.\n",
      "For a full field list, see aiR for Review results fields.\n",
      "Filtering and sorting aiR for Review results\n",
      "Documents have a one-to-many relationship with the aiR for Review's results fields. For example, a single document might be linked to five Issue results. This creates some limitations when sorting and filtering results:\n",
      "\n",
      "\n",
      "Filter one column at a time in the Document list. Combining filters may include more results than you expect.\n",
      "\n",
      "\n",
      "If you need to filter by more than one field at a time, we recommend using search conditions instead.\n",
      "\n",
      "\n",
      "You can add these fields to views and widgets, but you cannot sort the view or the widget by these fields.\n",
      "\n",
      "\n",
      "aiR for Review results fields\n",
      "The results of every aiR for Review analysis are stored as part of an analysis object. Each of the three result types has its own object type to match:\n",
      "\n",
      "\n",
      "aiR Relevance Analysis\n",
      "\n",
      "\n",
      "aiR Key Analysis\n",
      "\n",
      "\n",
      "aiR Issue Analysis\n",
      "\n",
      "\n",
      "aiR also links the results to each of the documents that were analyzed. These linked fields, called reflected fields, update to link to the newest results every time the document is analyzed. However, aiR keeps a record of all previous job results, and you can link the documents to a different job at any time. For more information, see Managing jobs and document linking.\n",
      "The reflected fields are the most useful for reviewing analysis results. These are formatted as aiR <review type> Analysis::<fieldname>. For example, the Prediction field for a Relevance analysis is called aiR Relevance Analysis::Prediction.\n",
      "aiR Relevance Analysis fields\n",
      "The fields for aiR Relevance Analysis are:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Field name\n",
      "Field type\n",
      "Description\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Name\n",
      "\n",
      "\n",
      "Fixed-length Text\n",
      "\n",
      "\n",
      "The name of this specific result. This formatted as <Document Artifact ID>_<Job ID>.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Job ID\n",
      "\n",
      "\n",
      "Fixed-length Text\n",
      "\n",
      "\n",
      "The unique ID of the job this result came from.\n",
      "\n",
      "\n",
      "\n",
      "Rank\n",
      "Whole Number\n",
      "Numerical rank indicating how strongly relevant the document is. For more information, see Understanding document ranks.\n",
      "\n",
      "\n",
      "Document\n",
      "Multiple Object\n",
      "The Control Number of the document this result is linked to. If the result is not currently linked to any documents, this field is blank.\n",
      "\n",
      "\n",
      "Prediction\n",
      "Fixed-length Text\n",
      "aiR's prediction of whether this qualifies as a relevant document.\n",
      "\n",
      "\n",
      "Rationale\n",
      "Fixed-length Text\n",
      "An explanation of why aiR chose this rank and prediction.\n",
      "\n",
      "\n",
      "Considerations\n",
      "Fixed-length Text\n",
      "A counterargument explaining why the rationale might possibly be wrong.\n",
      "\n",
      "\n",
      "Citation 1\n",
      "Fixed-length Text\n",
      "Excerpt from the document that supports the prediction and rationale. This may be blank for some documents.\n",
      "\n",
      "\n",
      "Citation 2\n",
      "Fixed-length Text\n",
      "Second excerpt from the document that supports the prediction and rationale. This may be blank for some documents.\n",
      "\n",
      "\n",
      "Citation 3\n",
      "Fixed-length Text\n",
      "Third excerpt from the document that supports the prediction and rationale. This may be blank for some documents.\n",
      "\n",
      "\n",
      "Citation 4\n",
      "Fixed-length Text\n",
      "Fourth excerpt from the document that supports the prediction and rationale. This may be blank for some documents.\n",
      "\n",
      "\n",
      "Citation 5\n",
      "Fixed-length Text\n",
      "Fifth excerpt from the document that supports the prediction and rationale. This may be blank for some documents.\n",
      "\n",
      "\n",
      "Error Details\n",
      "Fixed-length Text\n",
      "If the document encountered an error, the error message displays here. For an error list, see How document errors are handled.\n",
      "\n",
      "\n",
      "\n",
      "aiR Issues Analysis fields\n",
      "The fields for aiR Issues Analysis are:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Field name\n",
      "Field type\n",
      "Description\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Name\n",
      "\n",
      "\n",
      "Fixed-length Text\n",
      "\n",
      "\n",
      "The name of this specific result. This formatted as <Document ID>_<Job ID>.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Job ID\n",
      "\n",
      "\n",
      "Fixed-length Text\n",
      "\n",
      "\n",
      "The unique ID of the job this result came from.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Choice Analyzed\n",
      "\n",
      "\n",
      "Fixed Text\n",
      "\n",
      "\n",
      "The name of the issue choice being analyzed for this result.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Choice Analyzed ID\n",
      "\n",
      "\n",
      "Whole Number\n",
      "\n",
      "\n",
      "The Artifact ID of the issue choice being analyzed for this\n",
      " result.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Document\n",
      "\n",
      "\n",
      "Multiple Object\n",
      "\n",
      "\n",
      "The Control Number of the document this result is linked to. If the result is not currently linked to any documents, this field is blank.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Rank\n",
      "\n",
      "\n",
      "Whole Number\n",
      "\n",
      "\n",
      "Numerical rank indicating how well the document matches an issue. For more information, see Understanding document ranks.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Prediction\n",
      "\n",
      "\n",
      "Fixed-length Text\n",
      "\n",
      "\n",
      "aiR's predicted issue choice for this document.\n",
      "\n",
      "\n",
      "\n",
      "Rationale\n",
      "Fixed-length Text\n",
      "An explanation of why aiR chose this rank and prediction.\n",
      "\n",
      "\n",
      "Considerations\n",
      "Fixed-length Text\n",
      "A counterargument explaining why the rationale might possibly be wrong.\n",
      "\n",
      "\n",
      "\n",
      "Citation\n",
      "\n",
      "\n",
      "Fixed-length Text\n",
      "\n",
      "\n",
      "Excerpt from the document that supports the prediction and rationale. This may be blank for some documents.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Error Details\n",
      "\n",
      "\n",
      "Fixed-length Text\n",
      "\n",
      "\n",
      "If the document encountered an error, the error message displays here. For an error list, see How document errors are handled.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "aiR Key Analysis fields\n",
      "The fields for aiR Key Analysis are:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Field name\n",
      "Field type\n",
      "Description\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Name\n",
      "\n",
      "\n",
      "Fixed-length Text\n",
      "\n",
      "\n",
      "The name of this specific result. This formatted as <Document ID>_<Job ID>.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Job ID\n",
      "\n",
      "\n",
      "Fixed-length Text\n",
      "\n",
      "\n",
      "The unique ID of the job this result came from.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Document\n",
      "\n",
      "\n",
      "Multiple Object\n",
      "\n",
      "\n",
      "The Control Number of the document this result is linked to. If the result is not currently linked to any documents, this field is blank.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Rank\n",
      "\n",
      "\n",
      "Whole Number\n",
      "\n",
      "\n",
      "Numerical rank indicating how strongly relevant the document is. For more information, see Understanding document ranks.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Prediction\n",
      "\n",
      "\n",
      "Fixed-length Text\n",
      "\n",
      "\n",
      "aiR's prediction of whether this qualifies as a key document.\n",
      "\n",
      "\n",
      "\n",
      "Rationale\n",
      "Fixed-length Text\n",
      "An explanation of why aiR chose this rank and prediction.\n",
      "\n",
      "\n",
      "Considerations\n",
      "Fixed-length Text\n",
      "A counterargument explaining why the rationale might possibly be wrong.\n",
      "\n",
      "\n",
      "Citation 1\n",
      "Fixed-length Text\n",
      "Excerpt from the document that supports the prediction and rationale. This may be blank for some documents.\n",
      "\n",
      "\n",
      "Citation 2\n",
      "Fixed-length Text\n",
      "Second excerpt from the document that supports the prediction and rationale. This may be blank for some documents.\n",
      "\n",
      "\n",
      "Citation 3\n",
      "Fixed-length Text\n",
      "Third excerpt from the document that supports the prediction and rationale. This may be blank for some documents.\n",
      "\n",
      "\n",
      "Citation 4\n",
      "Fixed-length Text\n",
      "Fourth excerpt from the document that supports the prediction and rationale. This may be blank for some documents.\n",
      "\n",
      "\n",
      "Citation 5\n",
      "Fixed-length Text\n",
      "Fifth excerpt from the document that supports the prediction and rationale. This may be blank for some documents.\n",
      "\n",
      "\n",
      "\n",
      "Error Details\n",
      "\n",
      "\n",
      "Fixed-length Text\n",
      "\n",
      "\n",
      "If the document encountered an error, the error message displays here. For an error list, see How document errors are handled.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Using aiR for Review with Review Center\n",
      "One option for integrating aiR for Review into a larger review workflow is to combine it with Review Center. After analyzing the documents with aiR for Review, you can use aiR's predictions to prioritize which documents to include in a Review Center queue.\n",
      "For example, you may want to review all documents that aiR for Review ranked as borderline or above for relevance. To do that:\n",
      "\n",
      "\n",
      "Set up a saved search for documents where aiR Relevance Analysis::Rank is greater than 1. This returns all documents ranked 2 or higher.\n",
      "\n",
      "\n",
      "Create a Review Center queue using that saved search as the data source.\n",
      "\n",
      "\n",
      "Because of how the aiR for Review fields are structured, you cannot sort by them. However, you can either sort by another field, or use a prioritized review queue to dynamically serve up documents that may be most relevant.\n",
      "For more information, see Review Center.\n",
      "How document errors are handled\n",
      "If aiR encounters a problem when analyzing a document, it will not return results for that document. Instead, it ranks the document as -1 and returns an error message in the Error Details column. Your organization is not charged for any errored documents.\n",
      "The possible error messages are:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Error message\n",
      "Description\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Failed to parse completion \n",
      "The large language model (LLM) encountered an error. \n",
      "\n",
      "\n",
      "Completion is not valid JSON \n",
      "The large language model (LLM) encountered an error.\n",
      "\n",
      "\n",
      "Hallucination detected in completion \n",
      "The results for this document may include a hallucination. For more information, see Hallucinations and conflations.\n",
      "\n",
      "\n",
      "Conflation detected in completion \n",
      "The results for this document may include a conflation. For more information, see Hallucinations and conflations.\n",
      "\n",
      "\n",
      "Document text is empty \n",
      "The extracted text of the document was empty.\n",
      "\n",
      "\n",
      "Document text is too short \n",
      "There was not enough extracted text to analyze in the document.\n",
      "\n",
      "\n",
      "Document text is too long \n",
      "The document's extracted text was too long to analyze.\n",
      "\n",
      "\n",
      "Model API error occurred \n",
      "A communication error occurred between the large language model (LLM) and Relativity.\n",
      "\n",
      "\n",
      "Uncategorized error occurred \n",
      "An unknown error occurred.\n",
      "\n",
      "\n",
      "\n",
      "Hallucinations and conflations\n",
      "Two types of errors deserve special mention:\n",
      "\n",
      "\n",
      "Hallucinations—these occur when the aiR results citation cannot be found anywhere in the prompt text. The large language model (LLM) appears to be citing sentences that don't exist in the prompt.\n",
      "\n",
      "\n",
      "Conflations—these occur when the aiR results citation comes from something other than the document itself, but which is still part of the full prompt. For example, it might cite text that was part of the Prompt Criteria instead of the document's extracted text.\n",
      "\n",
      "\n",
      "When aiR receives the analysis results from the LLM, it checks all citations against the prompt text. Any possible hallucinations or conflations are marked as errors, and they receive a rank of -1 instead of whatever rank they were originally assigned. We recommend manually reviewing errored documents.\n",
      "Hallucinations are typically rare. However, highly structured documents such as Excel spreadsheets and PDF forms have a higher hallucination rate than other document types.\n",
      "Note: Due to the way that columns of text are scanned in OCR, OCR’d documents are occasionally marked as hallucinations when the citations do actually exist in the original document.\n",
      "\n",
      "aiR for Review security permissions\n",
      "This page contains information on the security permissions required for interacting with aiR for Review. For more information on setting permissions, see Workspace security.\n",
      "Note: aiR for Review is currently in limited release. For information about the general release, contact your account representative.\n",
      "See these related pages:\n",
      "\n",
      "aiR for Review\n",
      "\n",
      "Running aiR for Review\n",
      "\n",
      "Monitoring aiR for Review jobs\n",
      "\n",
      "aiR for Review results\n",
      "\n",
      "\n",
      "Running the aiR for Review mass action\n",
      "To run the aiR for Review mass action, you need the following permissions:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Object Security\n",
      "Other Settings\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "aiR for Review Profile - View, Edit, Add\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Mass Operations - aiR for Review\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "You must also belong to at least one user group other than the Workspace Admin Group.\n",
      "Viewing the aiR for Review Jobs tab\n",
      "There are two versions of the aiR for Review Jobs tab: one at the instance level, and one at the workspace level. The instance-level tab shows all jobs across all workspaces, and it includes several extra columns to identify the workspace, matter, and client connected to each job.\n",
      "The following permissions allow users to see the job list and click on each job to view Prompt Criteria details. Users with access to this tab can also cancel in-progress jobs.\n",
      "Instance-level permissions\n",
      "To view the instance-level aiR for Review Jobs tab, you need the following permissions:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Object Security\n",
      "Tab Visibility \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Admin Repository - View\n",
      "aiR for Review Prompt Criteria - View\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "aiR for Review Jobs\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Assign these permissions under the Instance Details tab.\n",
      "Viewing Prompt Criteria at the instance level\n",
      "To view Prompt Criteria details for a job, you also need some permissions within that job's workspace:\n",
      "\n",
      "You must belong to more than just the Workspace Admin Group within the workspace.\n",
      "You must have aiR for Review Prompt Criteria - View rights within that job's workspace.\n",
      "\n",
      "If you do not have these, you will be able to see jobs from that workspace, but you will not be able click on those jobs to view their Prompt Criteria.\n",
      "You can also use item-level permissions to restrict access to a specific job's aiR for Review Prompt Criteria. For more information, see Levels of Security in Relativity.\n",
      "Workspace-level permissions\n",
      "To view the workspace-level aiR for Review Jobs tab, you need the following permissions:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Object Security\n",
      "Tab Visibility \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "aiR for Review Prompt Criteria - View\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "aiR for Review Jobs\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Assign these permissions under the Workspace Details tab within the chosen workspace.\n",
      "You can also use item-level permissions to restrict access to a specific job's aiR for Review Prompt Criteria. For more information, see Levels of Security in Relativity.\n",
      "Clearing and restoring job results\n",
      "To clear or restore job results using the aiR for Review Jobs tab, you need the following permissions:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Object Security\n",
      "Tab Visibility \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Document - View, Edit\n",
      "aiR Relevance Analysis - View, Edit\n",
      "aiR Issue Analysis - View, Edit\n",
      "aiR Key Analysis - View, Edit\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "aiR for Review Jobs\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "If you have Edit permissions for only one of the analysis types, you will only be able to clear or restore results of that type.\n",
      "For more information on clearing and restoring results, see Managing jobs and document linking.\n",
      "Viewing highlights in the Viewer\n",
      "To  see aiR for Review results highlighted in the Viewer, you need the following permissions:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Object Security\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "aiR Relevance Analysis - View\n",
      "aiR Issue Analysis - View\n",
      "aiR Key Analysis - View\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "If you only have access to some of these, you will only see highlighting for those analysis types.\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# URL of the page you want to scrape\n",
    "base_url = \"https://help.relativity.com/RelativityOne/Content/Relativity/aiR_for_Review/aiR_for_Review.htm\"\n",
    "llm_input = ''\n",
    "\n",
    "# Send a GET request to the webpage\n",
    "response = requests.get(base_url)\n",
    "\n",
    "# Include the base_url\n",
    "if response.status_code == 200:\n",
    "    content = response.content\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    paragraphs = soup.find_all('div', id=\"mc-main-content\")\n",
    "    for paragraph in paragraphs:\n",
    "        llm_input += paragraph.get_text()\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage. Status code:\", response.status_code)\n",
    "\n",
    "# If the GET request is successful, the status code will be 200\n",
    "if response.status_code == 200:\n",
    "    # Get the content of the response\n",
    "    content = response.content\n",
    "\n",
    "    # Create a BeautifulSoup object and specify the parser\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "    # Find the sub-menu element\n",
    "    sub_menu = soup.find(class_='guideTOC')\n",
    "\n",
    "    # Find all anchor tags within the sub-menu and extract href attributes\n",
    "    links = [urljoin(base_url, a['href']) for a in sub_menu.find_all('a', href=True)]\n",
    "\n",
    "    # Loop through the links\n",
    "    for link in links:\n",
    "        # Send a GET request to the child page\n",
    "        response = requests.get(link)\n",
    "\n",
    "        # If the GET request is successful, the status code will be 200\n",
    "        if response.status_code == 200:\n",
    "            # Get the content of the response\n",
    "            content = response.content\n",
    "\n",
    "            # Create a BeautifulSoup object and specify the parser\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "            # Find the specified div tags\n",
    "            paragraphs = soup.find_all('div', id=\"mc-main-content\")\n",
    "\n",
    "            # Loop through the paragraphs, get the text of each one, and print it\n",
    "            for paragraph in paragraphs:\n",
    "                llm_input += paragraph.get_text()\n",
    "        else:\n",
    "            print(\"Failed to retrieve the webpage. Status code:\", response.status_code)\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage. Status code:\", response.status_code)\n",
    "\n",
    "print(llm_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# URL of the page you want to scrape\n",
    "base_url = \"https://help.relativity.com/RelativityOne/Content/Relativity/aiR_for_Review/aiR_for_Review.htm\"\n",
    "\n",
    "# Send a GET request to the webpage\n",
    "response = requests.get(base_url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    content = response.content\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    paragraphs = soup.find_all('div', id=\"mc-main-content\")\n",
    "    with open(os.path.join('results', f'aiR_for_Review.htm.txt'), 'w', encoding='utf-8') as f:\n",
    "        # Loop through the paragraphs, get the text of each one, and write it to the file\n",
    "        for paragraph in paragraphs:\n",
    "            f.write(paragraph.get_text())\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage. Status code:\", response.status_code)\n",
    "\n",
    "# If the GET request is successful, the status code will be 200\n",
    "if response.status_code == 200:\n",
    "    \n",
    "    # Get the content of the response\n",
    "    content = response.content\n",
    "\n",
    "    # Create a BeautifulSoup object and specify the parser\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "    # Find the sub-menu element\n",
    "    sub_menu = soup.find(class_='guideTOC')\n",
    "\n",
    "    # Find all anchor tags within the sub-menu and extract href attributes\n",
    "    links = [urljoin(base_url, a['href']) for a in sub_menu.find_all('a', href=True)]\n",
    "\n",
    "    # Loop through the links\n",
    "    for link in links:\n",
    "        # Send a GET request to the child page\n",
    "        response = requests.get(link)\n",
    "\n",
    "        # If the GET request is successful, the status code will be 200\n",
    "        if response.status_code == 200:\n",
    "            # Get the content of the response\n",
    "            content = response.content\n",
    "\n",
    "            # Create a BeautifulSoup object and specify the parser\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "            # Find the specified div tags\n",
    "            paragraphs = soup.find_all('div', id=\"mc-main-content\")\n",
    "\n",
    "            # Get the page name from the link\n",
    "            page_name = urlparse(link).path.split('/')[-1]\n",
    "\n",
    "            # Open a file with the page name\n",
    "            with open(os.path.join('results', f'{page_name}.txt'), 'w', encoding='utf-8') as f:\n",
    "                # Loop through the paragraphs, get the text of each one, and write it to the file\n",
    "                for paragraph in paragraphs:\n",
    "                    f.write(paragraph.get_text())\n",
    "        else:\n",
    "            print(\"Failed to retrieve the webpage. Status code:\", response.status_code)\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage. Status code:\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To run aiR for Review, you need the following permissions:\n",
      "\n",
      "1. **aiR for Review Profile - View, Edit, Add**: This permission allows you to interact with aiR for Review, including setting up, editing, and adding new profiles.\n",
      "\n",
      "2. **Mass Operations - aiR for Review**: This permission enables you to run mass operations related to aiR for Review, such as analyzing documents, setting up prompt criteria, and submitting jobs for analysis.\n",
      "\n",
      "Make sure you also belong to at least one user group other than the Workspace Admin Group for the system to recognize and authorize you to run aiR for Review.\n"
     ]
    }
   ],
   "source": [
    "#Pass the documentation text to GPT-3.5 Turbo and ask a question.\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": f\"You are a teacher helping me understand the following documentation on the aiR for Review product: {llm_input}\"},\n",
    "    {\"role\": \"user\", \"content\": f\"What permissions are needed to run aiR for Review?\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "assistant > To run the aiR for Review mass action, the necessary permissions are as follows:\n",
      "\n",
      "1. **Object Security:**\n",
      "   - aiR for Review Profile: View, Edit, Add permissions are needed.\n",
      "   \n",
      "2. **Other Settings:**\n",
      "   - Mass Operations: Permission specifically for aiR for Review must be granted.\n",
      "\n",
      "Additionally, you must be a member of at least one user group other than the Workspace Admin Group to perform actions related to aiR for Review. These permissions ensure authorized access for configuration and execution of reviews using the aiR for Review system within your workspace."
     ]
    }
   ],
   "source": [
    "assistant = client.beta.assistants.create(\n",
    "  name=\"Relativity Tutor\",\n",
    "  instructions=\"You are a personal tutor for the RelativityOne application. Review documentation and answer questions to help me learn.\",\n",
    "  model=\"gpt-4-turbo\",\n",
    ")\n",
    "\n",
    "thread = client.beta.threads.create()\n",
    "\n",
    "message = client.beta.threads.messages.create(\n",
    "  thread_id=thread.id,\n",
    "  role=\"user\",\n",
    "  content=f\"I have a question about {llm_input}. What permissions are needed to run aiR for Review?\"\n",
    ")\n",
    "\n",
    "from typing_extensions import override\n",
    "from openai import AssistantEventHandler\n",
    " \n",
    "# First, we create a EventHandler class to define\n",
    "# how we want to handle the events in the response stream.\n",
    " \n",
    "class EventHandler(AssistantEventHandler):    \n",
    "  @override\n",
    "  def on_text_created(self, text) -> None:\n",
    "    print(f\"\\nassistant > \", end=\"\", flush=True)\n",
    "      \n",
    "  @override\n",
    "  def on_text_delta(self, delta, snapshot):\n",
    "    print(delta.value, end=\"\", flush=True)\n",
    "      \n",
    "  def on_tool_call_created(self, tool_call):\n",
    "    print(f\"\\nassistant > {tool_call.type}\\n\", flush=True)\n",
    "  \n",
    "  def on_tool_call_delta(self, delta, snapshot):\n",
    "    if delta.type == 'code_interpreter':\n",
    "      if delta.code_interpreter.input:\n",
    "        print(delta.code_interpreter.input, end=\"\", flush=True)\n",
    "      if delta.code_interpreter.outputs:\n",
    "        print(f\"\\n\\noutput >\", flush=True)\n",
    "        for output in delta.code_interpreter.outputs:\n",
    "          if output.type == \"logs\":\n",
    "            print(f\"\\n{output.logs}\", flush=True)\n",
    " \n",
    "# Then, we use the `stream` SDK helper \n",
    "# with the `EventHandler` class to create the Run \n",
    "# and stream the response.\n",
    " \n",
    "with client.beta.threads.runs.stream(\n",
    "  thread_id=thread.id,\n",
    "  assistant_id=assistant.id,\n",
    "  instructions=\"Please address the user as Kelly. The user has a premium account.\",\n",
    "  event_handler=EventHandler(),\n",
    ") as stream:\n",
    "  stream.until_done()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do it without streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SyncCursorPage[Message](data=[Message(id='msg_y43WaQNzHmVYjipTDbSWqlNo', assistant_id='asst_EnPQXTGuBBXUbQd6AuHc2zO4', completed_at=None, content=[TextContentBlock(text=Text(annotations=[], value='To run aiR for Review, you will need specific permissions set up within your system. Here are the required permissions for running the aiR for Review mass action, based on the information provided:\\n\\n### Object Security\\n- **aiR for Review Profile:** You need to have View, Edit, and Add permissions.\\n\\n### Other Settings\\n- **Mass Operations:** Access to aiR for Review within the Mass Operations settings is required.\\n\\nAdditionally, to successfully operate aiR for Review, you must be part of at least one user group other than the Workspace Admin Group. This ensures that you have the appropriate level of access to handle sensitive data and utilize the aiR for Review tools effectively within your workspace.'), type='text')], created_at=1715285748, file_ids=[], incomplete_at=None, incomplete_details=None, metadata={}, object='thread.message', role='assistant', run_id='run_LVWhSu1sHfhpnEAce0BiMzeC', status=None, thread_id='thread_OJKDp8jDlTbP228xdcxzLqdo'), Message(id='msg_mdBAKl97CYSyvb9z5eWdY5Lc', assistant_id=None, completed_at=None, content=[TextContentBlock(text=Text(annotations=[], value='I have a question about \\naiR for Review\\naiR for Review harnesses the power of large language models, or LLMs, to review documents. aiR for Review goes far beyond existing classifiers by using generative AI to both predict coding decisions and to support those predictions with descriptive text and document excerpts which explain the decisions.\\nSome benefits of aiR for Review include:\\n\\n\\nHighly efficient, low-cost document analysis\\n\\n\\nQuick discovery of important issues and criteria\\n\\n\\nConsistent, cohesive analysis across all documents\\n\\n\\nNote: aiR for Review is currently in limited release. For information about the general release, contact your account representative.\\nSee these related pages:\\n\\nRunning aiR for Review\\n\\nMonitoring aiR for Review jobs\\n\\naiR for Review results\\n\\naiR for Review security permissions\\n\\n\\naiR for Review overview\\naiR for Review uses generative AI to simulate the actions of a human reviewer, finding and describing relevant documents according to the review instructions that you provide. It identifies the documents, describes why they are relevant using natural language, and demonstrates relevance using citations from the document.\\naiR for Review has three different analysis types:\\n\\n\\nRelevance review—predict documents responsive to a request for production.\\n\\n\\nIssues review—locate material relating to different legal issues.\\n\\n\\nKey documents—find hot documents important to a case or investigation.\\n\\n\\nSome use cases for aiR for Review include:\\n\\n\\nKickstarting the review process—prioritize the most important documents to give to reviewers.\\n\\n\\nFirst-pass review—determine what you need to produce and discover essential insights.\\n\\n\\nGaining early case insights—learn more about your matter right from the start.\\n\\n\\nInternal investigations—find documents and insights that help you understand the story hidden in your data.\\n\\n\\nAnalyzing productions from other parties—reduce the effort to find important material and get it into the hands of decision makers.\\n\\n\\nQuality control for traditional review—compare aiR for Review\\'s coding predictions to decisions made by reviewers to accelerate QC and improve results.\\n\\n\\naiR for Review workflow\\naiR for Review\\'s process is similar to training a human reviewer: explain the case and its relevance criteria, hand over the documents, and check the results. If aiR misunderstood any part of the relevance criteria, explain that part in more detail, then try again.\\nWithin Relativity, the main steps are:\\n\\n\\nSelect the documents to review\\n\\n\\nChoose the aiR for Review mass action\\n\\n\\nWrite and submit the review instructions, called Prompt Criteria\\n\\n\\nReview the results\\n\\n\\nWhen setting up the first analysis, we recommend running it on a sample set of documents that was already coded by human reviewers. If aiR\\'s predictions are different from the human coding, revise the Prompt Criteria and try again. This could include rewriting unclear instructions, defining an acronym or a code word, or adding more detail to an issue definition.\\nOverall, the workflow has three phases:\\n\\n\\nDevelop—write the Prompt Criteria, test, and tweak until the results match human review.\\n\\n\\nVerify—run the Prompt Criteria on a slightly larger set of documents and compare to results from senior reviewers.\\n\\n\\nRun—use the verified Prompt Criteria on a much larger set of documents.\\n\\n\\nFor more details, see Running aiR for Review. For additional workflow help and examples, see Workflows for Applying aiR for Review on the Community site.\\nHow aiR for Review works\\naiR for Review\\'s analysis is powered by Azure OpenAI\\'s GPT-4 large language model, or LLM. The LLM is designed to understand and generate human language, and it is trained on billions of documents from open datasets and the web.\\nWhen you submit Prompt Criteria and a set of documents to aiR for Review, Relativity sends the first document to Azure OpenAI and asks it to review the document according to the Prompt Criteria. After Azure OpenAI returns its results, Relativity sends the next document. The LLM reviews each document independently, and it does not learn from previous documents. Unlike Review Center, which makes its predictions based on learning from the document set, the LLM makes its predictions based on the Prompt Criteria and its preexisting training.\\nAzure OpenAI does not retain any data from the documents being analyzed. Data you submit for processing by Azure OpenAI is not retained beyond your organization’s instance, nor is it used to train any other generative AI models from Relativity, Microsoft, or any other third party. For more information, see the white paper A Focus on Security and Privacy in Relativity’s Approach to Generative AI.\\nFor more information on using generative AI for document review, we recommend:\\n\\n\\nRelativity Webinar - AI Advantage: How to Accelerate Review with Generative AI\\n\\n\\n\\nMIT\\'s Generative AI for Law resources\\n\\n\\n\\nThe State Bar of California\\'s drafted recommendations for the use of generative AI\\n\\n\\n\\nUsing aiR for Review with non-English languages\\naiR for Review currently supports English-language Prompt Criteria and analyzing English text. It has not been tested for use with other languages, and doing so may result in unexpected or unverifiable results. However, Relativity will continue to do additional testing and analysis on non-English language scenarios.\\nUsing aiR for Review with emojis\\naiR for Review has not been specifically tested for analyzing emojis. However, the underlying LLM does understand Unicode emojis. It also understands other formats that could normally be understood by a human reviewer. For example, an emoji that is extracted to text as :smile: would be understood as smiling.\\nArchiving and restoring workspaces with aiR for Review\\nWorkspaces with aiR for Review installed can be archived and restored using the ARM application.\\nWhen archiving, check Include Extended Workspace Data under Extended Workspace Data Options. If this option was not checked during the archive process, the aiR for Review features in the restored workspace will not be fully functional.  If this happens, you will need to manually reinstall aiR for Review in the restored workspace.\\nFor more information on using ARM, see ARM.\\n\\xa0\\n\\nRunning aiR for Review\\naiR for Review uses generative AI to simulate the actions of a human reviewer, finding and describing relevant documents using the review instructions that you provide. These analyses can be customized to search for relevance, key documents, or specific issues as needed.\\nThe instructions you give aiR for Review are called Prompt Criteria. For best results, we recommend analyzing a small set of documents, tweaking the Prompt Criteria as needed, then finally analyzing a larger set of documents. This lets you see immediately how aiR\\'s coding compares to a human reviewer\\'s coding and adjust the prompts accordingly.\\nNote: aiR for Review is currently in limited release. For information about the general release, contact your account representative.\\nSee these related pages:\\n\\naiR for Review\\n\\nMonitoring aiR for Review jobs\\n\\naiR for Review results\\n\\naiR for Review security permissions\\n\\n\\nSee these additional resources:\\n\\nWorkflows for Applying aiR for Review\\n\\naiR for Review example project\\n\\n\\nInstalling aiR for Review\\naiR for Review is available as a secured application from the Application Library. You must have an active aiR for Review contract to use it, and it is not available for repository workspaces.\\nTo install it:\\n\\n\\nNavigate to the Relativity Applications tab in your workspace.\\n\\n\\nSelect Install from application library.\\n\\n\\nSelect the aiR for Review application.\\n\\n\\nClick Install.\\n\\n\\nAfter installation completes, the following object types will appear in your workspace:\\n\\n\\naiR Relevance Analysis—records the Relevance results of aiR for Review analysis runs.\\n\\n\\naiR Issue Analysis—records the Issue results of aiR for Review analysis runs.\\n\\n\\naiR Key Analysis—records the Key results of aiR for Review analysis runs.\\n\\n\\naiR for Review Prompt Criteria—records the Prompt Criteria settings and contents for each analysis run. This also records Prompt Criteria drafts for each user.\\n\\n\\nInstalling aiR for Review creates two versions of the aiR for Review Jobs tab: one at the instance level, and one at the workspace level. Some users also find it helpful to manually create tabs for the three Analysis objects, but this is optional.\\nFor more information on installing applications, see Relativity applications.\\nSetting up permissions\\nFor detailed information on aiR for Review user permissions, see aiR for Review security permissions.\\nChoosing an analysis type\\naiR for Review supports three types of analysis. Each one is geared towards a different phase of a review or investigation.\\n For each aiR for Review job, choose one analysis type:\\n\\n\\nRelevance—analyzes whether documents are relevant to a case or situation that you describe, such as documents responsive to a production request.\\n\\n\\nRelevance and Key Documents—analyzes documents for both relevance and whether they are “hot” or key to a case.\\n\\n\\nIssues—analyzes documents for whether they include content that falls under specific categories. For example, you might use this to check whether documents involve coercion, retaliation, or a combination of both.\\n\\n\\nBased on the analysis type you choose, you will need the following fields:\\n\\n\\nRelevance—one single-choice results field. The field must have at least one choice.\\n\\n\\nRelevance and Key Documents—two single-choice results fields. These should have distinct names such as \"Relevant\" and \"Key,\" and each field should have at least one choice.\\n\\n\\nIssues—one multi-choice results field. Each choice should represent one of the issues you want to analyze.\\nNote: Currently, aiR for Review analyzes a maximum of five issues per run. You can have as many choices for the field as you want, but you can only analyze five at a time.\\n\\n\\naiR for Review does not actually write to these fields. Instead, it uses them as reference when making and reporting on its predictions.\\nChoosing a processing mode\\naiR for Review supports two processing modes: Fast Track and Batch. These are designed for the workflow of fine-tuning Prompt Criteria on a small set of documents, then using the Prompt Criteria on a large set of documents.\\nBecause the large language model (LLM) has limited capacity, we dedicate some bandwidth exclusively to Fast Track jobs. This returns speedy results for smaller jobs without them having to wait for larger jobs in the queue.\\n For each aiR for Review job, choose one mode:\\n\\n\\nFast Track—processes up to 50 documents quickly. Use this to test and refine your Prompt Criteria on a small set of test documents.\\n\\n\\nEach user can have only one Fast Track job running at a time.\\n\\n\\nThese jobs use dedicated bandwidth that is not available to larger batch jobs. They typically return results within 5-10 minutes.\\n\\n\\n\\n\\nBatch—processes up to 50,000 documents. Use this to run your previously refined Prompt Criteria on a larger set of documents.\\n\\n\\nEach instance can have up to three Batch jobs running at a time.\\n\\n\\nProcessing time varies based on document load and total load on the LLM.\\n\\n\\n\\n\\nFor more detailed information about each mode’s capacity, see Job capacity and size limitations.\\nBest practices for running aiR for Review\\naiR for Review works best after fine-tuning the Prompt Criteria. Analyzing just a few documents at first, comparing the results to human coding, and then adjusting the Prompt Criteria as needed yields more accurate results than diving in with a full document set.\\nWe recommend the following workflow:\\n\\n\\nFor your first analysis, run the Prompt Criteria on a set of 10 test documents that are a mix of relevant and  not relevant.\\n\\n\\nCompare the results to human coding. In particular, look for documents that aiR coded differently than the humans did and investigate possible reasons. This could include unclear instructions, needing to define an acronym or code word, or other blind spots in the Prompt Criteria.\\n\\n\\nTweak the Prompt Criteria to adjust for blind spots.\\n\\n\\nRepeat steps 1 through 3 until aiR predicts coding decisions accurately for the test documents.\\n\\n\\nTest the Prompt Criteria on 50 documents and compare results. Continue tweaking as needed.\\n\\n\\nFinally, run the Prompt Criteria on a larger set of documents.\\n\\n\\naiR only sees the extracted text of a document. It does not see any non-text elements like advanced formatting, embedded images, or videos. We do not recommend using aiR for Review on documents such as images, videos, or spreadsheets with heavy formulas. Instead, use it on documents whose extracted text accurately represents their content and meaning.\\nRunning the analysis\\naiR for Review works as a mass action found on the Documents tab. Running the analysis has three basic parts:\\n\\n\\nSelecting documents and setting up the review\\n\\n\\nWriting the Prompt Criteria\\n\\n\\nSubmitting the job for analysis\\n\\n\\nAt any point in this process, you can click Save and Close in the mass action modal. This saves your progress so that you can keep working on it at a later time.\\nWhen you reopen the mass action modal, the last Prompt Criteria that you saved will display. For more information, see Running aiR for Review.\\nStep 1: Selecting documents and setting up the review\\nTo start an aiR for Review analysis job:\\n\\n\\nFrom the Documents tab, select the documents you want to analyze.\\n\\n\\nUnder Mass Actions, select aiR for Review. A modal with several tabs appears.\\n\\n\\nOn the Setup tab of the modal, set the following:\\n\\n\\nPrompt Criteria Name—give the Prompt Criteria a unique name. You can also click Load Prompt Criteria to select Prompt Criteria that you or another user previously wrote. For more information, see Editing and collaboration.\\n\\n\\nReview Type—select one of the following. For more information, see Choosing an analysis type.\\n\\nRelevance—analyzes whether documents are relevant to a case or situation that you describe, such as documents responsive to a production request.\\n\\nRelevance and Key Documents—analyzes documents for both relevance and whether they are “hot” or key to a case.\\n\\n\\nIssues—analyzes documents for whether they include content that falls under specific categories.\\n\\n\\n\\n\\nRun in Fast Track—toggle this On for 50 documents or fewer, and Off for more than 50 documents. For more information, see Choosing a processing mode.\\n\\n\\n\\n\\nThe first time you use the mass action, all the fields will be blank. When you click Save and Next or Save and Close, your progress saves and persists for the next analysis.\\nIf any required fields on any of the tabs are empty or misconfigured, the Save and Next button will be unavailable. Click on the title of each tab to fill out its fields.\\nStep 2: Writing the Prompt Criteria\\nThe Prompt Criteria are a set of inputs that give aiR the context it needs to understand the matter and evaluate each document. Writing the Prompt Criteria is a way of training your \"reviewer,\" similar to training a human reviewer.\\nDepending which type of analysis you chose, you will see a different set of tabs. All Prompt Criteria include the Case Summary tab.\\nGeneral writing guidelines\\nFor all of the setup tabs, we recommend:\\n\\n\\nWrite as if \"less is more.\" Instead of pasting in a long review protocol as-is, summarize where possible and include only key passages. The Prompt Criteria have an overall length limit of 10,000 characters.\\n\\n\\nPhrase things in a positive way when possible. Avoid negatives (\"not\" statements) and double negatives.\\n\\n\\nDo not include explanations of the law.\\n\\n\\nDo not give the LLM commands, such as “you will review XX.\" Instead, simply describe the case.\\n\\n\\nUse whatever writing format makes the most sense to a human reader. For example, bullet points might be useful for the People and Aliases section, but paragraphs might make sense in another section.\\n\\n\\nThe LLM has essentially “read the whole Internet.” It understands widely used slang and abbreviations, but it does not necessarily know jargon or phrases that are specific to an organization.\\n\\n\\nWhen you start to write your first Prompt Criteria, the fields contain grayed-out helper text that shows examples of what to enter. Use this as a guideline for crafting your own entries.\\nNote: \\nFor more guidance on prompt writing, see the following resources on the Community site:\\n\\n\\naiR for Review Prompt Writing Best Practices—downloadable PDF of writing guidelines\\n\\n\\naiR for Review example project—detailed example of adapting a review protocol into Prompt Criteria\\n\\n\\n\\nFilling out the Case Summary tab\\nThe Case Summary gives the broad context surrounding a matter. It includes an overview of the matter, people and entities involved, and any jargon or terms that are needed to understand the document set.\\nLimit the Case Summary content to roughly 20 sentences overall, and 20 each of People and Aliases, Noteworthy Organizations, and Noteworthy Terms.\\nTo fill out the Case Summary tab:\\n\\n\\nWithin the setup modal, click on the Case Summary tab.\\n\\n\\nFill out the following:\\n\\n\\nMatter Overview—provide a concise overview of the case. Include the names of the plaintiff and defendant, the nature of the dispute, and other important case characteristics.\\n\\n\\nPeople and Aliases—list the names and aliases of key custodians who authored or received the documents. Include their role and any other affiliations.\\n\\n\\nNoteworthy Organizations—list the organizations and other relevant entities involved in the case. Highlight any key relationships or other notable characteristics.\\n\\n\\nNoteworthy Terms—list and define any relevant words, phrases, acronyms, jargon, or slang that might be important to the analysis.\\n\\n\\nAdditional Context—list any additional information that does not fit the other fields. This section is typically left blank.\\n\\n\\n\\n\\nDepending on which Review Type you chose, the remaining tabs will be called Relevance, Key Documents, or Issues. Fill out those tabs according to the guide sections below.\\nFilling out the Relevance tab\\nIf you chose either Relevance or Relevance and Key Documents as the Review Type, you will see the Relevance tab. This defines the fields and criteria used for determining if a document is relevant to the case.\\nTo fill out the Relevance tab:\\n\\n\\nWithin the setup modal, click on the Relevance tab.\\n\\n\\nFill out the following:\\n\\n\\nRelevance Field—select a single-choice field that represents whether a document is relevant or non-relevant.\\n\\n\\nRelevant Choice—select the field choice you use to mark a document as relevant.\\n\\n\\nRelevance Criteria—summarize the criteria that determine whether a document is relevant. Include:\\n\\n\\nKeywords, phrases, legal concepts, parties, entities, and legal claims\\n\\n\\nAny criteria that would make a document non-relevant, such as relating to a project that is not under dispute\\n\\n\\n\\n\\nIssues Field (Optional)—select a single-choice or multi-choice field that represents the issues in the case.\\n\\n\\nChoice Criteria—select each of the field choices one by one. For each choice, write a summary in the text box listing the criteria that determine whether that issue applies to a document. For more information, see Filling out the Issues tab.\\n\\n\\nNote: aiR does not make Issue predictions during Relevance review, but you can use this field for reference when writing the Relevance Criteria. For example, you could tell aiR that any documents related to these issues are relevant.\\n\\n\\n\\n\\nFor best results when writing the Relevance Criteria:\\n\\n\\nLimit the Relevance Criteria to 5-10 sentences.\\n\\n\\nDo not paste in the original request for production (RFP); those are often too long and complex to give good results. Instead, summarize it and include key excerpts.\\n\\n\\nGroup similar criteria together when you can. For example, if an RFP asks for “emails pertaining to X” and “documents pertaining to X,” write “emails or documents pertaining to X.”\\n\\n\\nFilling out the Key Documents tab\\nIf you chose Relevance and Key as the Review Type, you will see the Key Documents tab. This defines the fields and criteria used for determining if a document is \"hot\" or key to the case.\\nTo fill out the Key Documents tab:\\n\\n\\nWithin the setup modal, click on the Key Documents tab.\\n\\n\\nFill out the following:\\n\\n\\nKey Document Field—select a single-choice field that represents whether a document is key to the case.\\n\\n\\nKey Document Choice—select the field choice you use to mark a document as key.\\n\\n\\nKey Document Criteria—summarize the criteria that determine whether a document is key. Include:\\n\\n\\nKeywords, phrases, legal concepts, parties, entities, and legal claims\\n\\n\\nAny criteria that would exclude a document from being key, such as falling outside a certain date range\\n\\n\\n\\n\\n\\n\\nFor best results, limit the Key Document Criteria to 5-10 sentences.\\nFilling out the Issues tab\\nIf you chose Issues as the Review Type, you will see the Issues tab. This defines the fields and criteria used for determining whether a document relates to a set of specific topics or issues.\\nTo fill out the Issues tab:\\n\\n\\nWithin the setup modal, click on the Issues tab.\\n\\n\\nFill out the following:\\n\\n\\nField—select a multi-choice field that represents the issues in the case.\\n\\n\\nChoice Criteria—select each of the field choices one by one. For each choice, write a summary in the text box listing the criteria that determine whether that issue applies to a document. Include:\\n\\n\\nKeywords, phrases, legal concepts, parties, entities, and legal claims\\n\\n\\nAny criteria that would exclude a document from being key, such as falling outside a certain date range\\n\\n\\n\\n\\n\\n\\nFor best results when writing the Choice Criteria:\\n\\n\\nLimit the criteria description for each choice to 5-10 sentences.\\n\\n\\nEach of the choices must have its own criteria. If a choice has no criteria, either fill it in or remove the choice.\\n\\n\\nRemoving issue choices\\naiR analyzes a maximum of 5 choices. If the issue field has more than 5 choices:\\n\\n\\nSelect the choice you want to remove.\\n\\n\\nClick the Remove Choice button on the right.\\n\\n\\nRepeat with any other unwanted choices.\\n\\n\\nStep 3: Submitting the job for analysis\\nAfter filling out the Setup, Case Summary, and other tabs, review the job and submit it for analysis.\\nTo submit a job:\\n\\n\\nClick Save and Next.\\n\\n\\nReview the confirmation summary. This includes:\\n\\n\\nTotal Docs—number of documents to be analyzed.\\n\\n\\nEst. Total Doc Units—number of document units counted for billing purposes. For more information, see How document units are calculated.\\n\\n\\nEst. Time to Start—estimated wait time from when you submit the job, to when aiR can begin analyzing your job. Longer wait times appear when aiR already has other work queued up.\\n\\n\\nEst. Run Time—estimated time aiR will take to analyze and return the results of the documents selected. This does not include time waiting in the queue.\\n\\n\\n\\n\\nClick Start Classification.\\nA banner appears showing that the job was successfully submitted for analysis.\\n\\n\\nNote: If you try to run a job that is too large or when too many jobs are already running, an error will appear. You can still save and edit the Prompt Criteria, but you will not be able to start the job. For more information, see Job capacity and size limitations.\\nFor information on monitoring jobs in progress, see Monitoring aiR for Review jobs.\\nEditing and collaboration\\nBy default, when you select the mass action, the last Prompt Criteria you saved will display. This makes it easy to edit the Prompt Criteria without re-entering information.\\nIf you want to edit a different set of Prompt Criteria or collaborate with another user, you can load previous Prompt Criteria into the mass action modal. From there, any edits you make will be saved as a new set of Prompt Criteria.\\nTo load previous Prompt Criteria:\\n\\n\\nFrom the Documents tab, check the documents you want to analyze.\\n\\n\\nUnder Mass Actions, select aiR for Review.\\n\\n\\nOn the Setup tab of the modal,  click Load Prompt Criteria. A pop-up opens with two tabs:\\n\\n\\nPrompt Criteria—Prompt Criteria for jobs that already ran in the workspace.\\n\\n\\nDrafts—each user’s most recently saved Prompt Criteria. These may or may not have been run yet.\\n\\n\\n\\n\\nSelect a row from either tab.\\nThe right-hand panel shows a preview of the Prompt Criteria.\\n\\n\\nClick Load.\\nThe Prompt Criteria Name, Analysis Type, and all criteria load into the aiR for Review modal.\\n\\n\\nIf you run the previous Prompt Criteria without making any changes, the Prompt Criteria Name stays the same. If you edit the prompt before running it, a number such as (1) or (2) will be automatically added to the end of the Prompt Criteria Name. You can also manually enter your own name for the Prompt Criteria as you edit it.\\nIf you and another user both edit the same Prompt Criteria at the same time, your edits are saved as separate drafts. To collaborate on the same draft, we recommend having the first user finish their edits, then pass the draft off to the second user.\\nNote: Only one Prompt Criteria draft is saved for each user. If you save a draft, then load in different Prompt Criteria, that draft will be overwritten. To save Prompt Criteria long-term, run them with one or more documents.\\nHow document units are calculated\\nA document unit is a document with between 1 and 15,000 characters of text. If a document has more than 15,000 characters in it, it is counted as two or more document units. For example, a 1-character document counts as one document unit, but a 16,000-character document counts as two document units.\\nNote: Any Unicode character counts as one character, regardless of storage size.\\nBecause the document unit estimation calculates white space slightly differently than actual billing, small discrepancies may appear for document sizes that are right on the border between two document units. To find the actual document units that are billed, see the Cost Explorer.\\nJob capacity and size limitations\\nBased on the limits of the underlying large language model (LLM), aiR has size limits for the documents and prompts you submit, as well as volume limits for the overall jobs.\\nSize limits\\nThe documents and Prompt Criteria have the following size limits:\\n\\n\\nThe Prompt Criteria have an overall length limit of 10,000 characters.\\n\\n\\nEach document\\'s extracted text should be under 150KB if possible. aiR has a hard limit of 300KB extracted text per document, but due to how the LLM processes document size, it sometimes rejects documents that are smaller than 300KB. To avoid this, we recommend treating 150KB as the upper limit.\\n\\n\\nEach document\\'s extracted text, when combined with the Prompt Criteria, must be less than 32,000 “tokens” (roughly equivalent to words, symbols, or whitespace). This is usually not a problem for documents under 150KB.\\n\\n\\nVolume limits\\nThe volume limits for aiR for Review jobs are as follows:\\n\\n\\n\\n\\n\\n\\nVolume Type\\nLimit\\nNotes\\n\\n\\n\\n\\nMax job size for Fast Track mode\\n50 documents\\nFor over 50 documents, use Batch mode.\\n\\n\\n Max job size for Batch mode \\n50,000 documents\\n\\nA single Batch job can include up to 50,000 documents.\\n\\n\\n\\nConcurrent Fast Track jobs per user\\n1 job\\nEach user can only have one Fast Track job running at a time. However, multiple users in one instance can run Fast Track jobs at the same time.\\n\\n\\nConcurrent Batch jobs per instance\\n3 jobs\\nOnly 3 Batch jobs can be queued or running at the same time within an instance.\\n\\n\\n\\nSpeed\\nAfter a job is submitted, aiR analyzes roughly 25-50 documents per minute. Fast Track jobs typically take under 10 minutes. Batch job speeds vary widely depending on the number of documents, the overall load on the LLM, and other factors.\\n\\nMonitoring aiR for Review jobs\\nAfter an aiR for Review job has started, you can use the aiR for Review jobs tab to monitor its progress, view prompt details, or cancel it. You can also view completed jobs and choose which analysis results are connected to the documents.\\nFor Fast Track jobs, banner statuses appear on the Documents tab to let you know when a job is complete. You can also monitor these from the aiR for Review jobs tab like any other job.\\nNote: aiR for Review is currently in limited release. For information about the general release, contact your account representative.\\nSee these related pages:\\n\\naiR for Review\\n\\nRunning aiR for Review\\n\\naiR for Review results\\n\\naiR for Review security permissions\\n\\n\\naiR for Review Jobs tab\\nThere are two versions of the aiR for Review Jobs tab: one at the instance level, and one at the workspace level. The instance-level tab shows all jobs across all workspaces, and it includes several extra columns to identify the workspace, matter, and client connected to each job. Most users only need access to the workspace-level tab. However, because some of aiR\\'s volume limits are instance-wide, the instance-level tab makes it easy to see exactly how much capacity is being used.\\nBoth versions of the tab show aiR for Review jobs that have been submitted for analysis. You can use the tab to view prompt details, cancel queued or in-progress jobs, and manage the job results.\\nFor information on managing tab permissions, see aiR for Review security permissions.\\nNote: If the aiR for Review Jobs tab says that aiR for Review is not currently available, check with your administrator. Your organization might not have an active contract for aiR for Review.\\nManaging jobs and document linking\\nYou can use the aiR for Review Jobs tab to cancel jobs, clear job results out of the document fields, and restore previous job results.\\nTo manage jobs, use the following icons:\\n\\n\\nCancel symbol (\\n)—cancels a queued or in-progress job. Any results that were already received from the large language model (LLM)\\xa0will stay in the fields, and those results will still be billed.\\n\\n\\nClear symbol (\\n)—clears job results from the documents in this run. This empties the aiR for Review fields and removes highlighting from the Viewer, but it does not permanently delete the results. The results can be restored and re-linked at any time.\\n\\n\\nRestore symbol (\\n)—re-links the results of the selected job to the documents in the run. This replaces the results of any other job with the same result type.\\n\\n\\nFor example, if you realize your current Prompt Criteria gives you less helpful results than a previous Prompt Criteria did, you can restore the previous job\\'s results. This immediately gives reviewers access to the old predictions without needing to re-run the old Prompt Criteria.\\nNotes: \\nIf you run a new job on documents that were part of a previous job, you may temporarily see both sets of results linked to those documents. The old results will be unlinked after the new job is complete.\\n\\nTo avoid seeing doubled results, clear the previous result set using the aiR for Review Jobs tab.\\n\\n\\nViewing job details\\nTo see an aiR for Review job\\'s Prompt Criteria, click on its row. A detail panel opens showing the setup details, case summary, fields, and criteria for analysis.\\nYou can control a user\\'s access to the detail panel using both item-level and workspace-level permissions. For more information, see aiR for Review security permissions.\\nJobs tab fields\\nThe following fields appear on the aiR for Review Jobs tab:\\n\\n\\nJob ID—the unique ID assigned to a job.\\n\\n\\nPrompt Criteria Name—the name of the Prompt Criteria used by the job.\\n\\n\\nIf several jobs ran using the same Prompt Criteria, this name will be the same for those jobs.\\n\\n\\nIf a user edited the Prompt Criteria before running the job but did not change the name, the Prompt Criteria name will have a version number such as (1) or (2) appended after it.\\n\\n\\n\\n\\nJob Status—the current state of the job. The possible statuses are:\\n\\n\\nNot Started\\n\\n\\nQueued\\n\\n\\nIn Progress\\n\\n\\nCompleted\\n\\n\\nCancelling\\n\\n\\nErrored\\n\\n\\n\\n\\nJob Type—the job\\'s processing mode. For more information, see Choosing a processing mode.\\n\\n\\nClient Name (workspace-level only)—the client associated with the job\\'s workspace.\\n\\n\\nMatter Name (workspace-level only)—the matter name associated with the job\\'s workspace.\\n\\n\\nMatter Number (workspace-level only)—the matter number associated with the job\\'s workspace.\\n\\n\\nWorkspace ID (workspace-level only)—the ID of the job\\'s workspace.\\n\\n\\nWorkspace Name (workspace-level only)—the name of the job\\'s workspace.\\n\\n\\nDoc Count—the number of documents submitted for analysis.\\n\\n\\nDocs Successful—the number of documents that were successfully analyzed.\\n\\n\\nDocs Pending—the number of documents that are waiting to be analyzed.\\n\\n\\nDocs Errored—the number of documents that encountered an error during analysis.\\n\\n\\nDocs Skipped—the number of documents that aiR\\xa0did not return results for. This can happen for reasons such as cancelling a job, network errors, and partial or complete job failures.\\n\\n\\nUser Name—the user who submitted the job.\\n\\n\\nSubmitted Time—the time the user submitted the job.\\n\\n\\nCompleted Time—the time the job successfully completed. If the job failed or was cancelled early, this field is blank.\\n\\n\\nTerminated Time—the time the job stopped running, regardless of whether it was cancelled, failed, or completed successfully.\\n\\n\\nJob Failure Reason—if the job failed, the reason is listed here. If the job completed successfully, this field is blank.\\n\\n\\nEstimated Wait Time—the initial estimate for how long the job will wait between when the user submits the job and when the job can start running.\\n\\n\\nEstimated Run Time—the initial estimate for how long the job will take to run.\\n\\n\\nDocument Units—the number of documents counted for billing purposes. For more information, see How document units are calculated.\\n\\n\\nFast Track banner statuses\\nWhen you submit a Fast Track job, the confirmation banner on the Documents tab updates automatically to show you the status of the job. This makes it easier to keep track of a small job\\'s status without needing to check the dedicated Jobs tab. This banner only appears for the user who submitted the job.\\nThe banner updates when the job is queued, in progress, and complete.\\n\\xa0\\n\\naiR for Review results\\nWhen aiR for Review analyzes documents, it makes predictions about the relevance of documents to different topics or issues. If it predicts that a document is relevant or relates to an issue, it includes a written justification of that prediction, as well as a counterargument and in-text citations. You can view these predictions, citations, and justifications either from the Viewer, or from a custom document view.\\nNote: aiR for Review is currently in limited release. For information about the general release, contact your account representative.\\nSee these related pages:\\n\\naiR for Review\\n\\nRunning aiR for Review\\n\\nMonitoring aiR for Review jobs\\n\\naiR for Review security permissions\\n\\n\\nHow aiR for Review results work\\nWhen aiR for Review finishes its analysis, it returns a prediction about how each document should be categorized, as well as its reasons for that prediction. This analysis has several parts:\\n\\n\\nRank—a numerical rank that indicates how strongly relevant the document is or how well it matches the predicted issue.\\n\\n\\nPrediction—the relevance, key, or issue label that aiR predicts should apply to the document.\\n\\n\\nRationale—an explanation of why aiR chose this rank and prediction.\\n\\n\\nConsiderations—a counterargument explaining why the rationale might possibly be wrong.\\n\\n\\nCitations—excerpts from the document that support the prediction and rationale.\\n\\n\\nIn general, citations are left empty for non-relevant documents and documents that don\\'t match an issue. However, aiR occasionally provides a citation for low-ranking documents if it helps to clarify why it was marked non-relevant. For example, if aiR is searching for changes of venue, it might cite an email that ends with \"Hang on, gotta run, more later\" as worth noting, even though it understands that’s not a true change of venue request.\\nPredictions versus document coding\\nEven though aiR refers to the relevance, key, and issue fields during its analysis, it does not actually write to these fields. All of aiR\\'s results are stored in aiR-specific fields such as the Prediction field. We recommend using these aiR fields for reference and reserving the actual relevance, key, and issue fields for human coding.\\nFor ideas on how to integrate aiR for Review results into a larger review workflow, see Using aiR for Review with Review Center.\\nVariability of results\\nBecause of how large language models work, results can vary slightly from run to run. aiR\\'s results for an individual document can potentially change even when given the same set of inputs. However, this is relatively rare; from our testing, it happens about 4% of the time.\\nUnderstanding document ranks\\naiR ranks documents from 0 to 4 according to how relevant they are or how well they match an issue. The higher the number, the more relevant the document is predicted to be. In addition, aiR assigns a rank of -1 to any errored documents. Because these were not properly analyzed, they cannot receive a normal rank.\\nThe aiR for Review ranks are:\\n\\n\\n\\n\\n\\nRank\\nDescription\\n\\n\\n\\n\\n-1\\nThe document either encountered an error or could not be analyzed. For more information, see How document errors are handled.\\n\\n\\n0 \\nThe document is “junk” data such as system files or sets of random characters.\\n\\n\\n1\\nThe document is predicted not relevant. aiR did not find any evidence that it relates to the case or issue.\\n\\n\\n2\\nThe document is predicted borderline relevant. aiR found some content that might relate to the case or issue. It usually has citations.\\n\\n\\n3\\nThe document is predicted relevant to the issue. Citations show the relevant text.\\n\\n\\n4\\nThe document is predicted very relevant to the issue. aiR found direct, strong evidence that the content relates to the case or issue. Citations show the relevant text.\\n\\n\\n\\nViewing results for individual documents\\nFrom the Viewer, you can see the aiR for Review results for each individual document. Predictions show up in the left-hand pane, and all citations are automatically highlighted.\\nTo view a document\\'s aiR for Review results, click on the aiR for Review Analysis icon () to expand the pane. The aiR for Review Analysis pane displays the following:\\n\\n\\nAnalysis Name\\n\\n\\nPrediction\\n\\n\\nRationale and Considerations\\n\\n\\nCitation\\n\\n\\nNotes: \\nIf you run a new job on documents that were part of a previous job, you may temporarily see both sets of results linked to those documents. The old results will be unlinked after the new job is complete.\\n\\nTo avoid seeing doubled results, clear the previous result set using the aiR for Review Jobs tab.\\n\\n\\nCitations and highlighting\\nTo jump to a specific citation, click the citation card. You can also toggle highlighting on or off by clicking the toggle at the top of the aiR for Review Analysis pane.\\nThe highlight colors depend on the type of citation:\\n\\n\\nRelevance citation—orange.\\n\\n\\nKey Document citation—purple.\\n\\n\\nIssue citation—color set chosen in the Color Map application. For more information, see Changing the color associated with a coding choice.\\n\\n\\nIf the same passage is cited by two types of results, the highlight blends their colors.\\nAdding aiR for Review fields to layouts\\nBecause of how aiR for Review results fields are structured, you cannot add them directly to layouts. If the highlighting is not enough, you can add an object list to the layout that shows all linked results. For more information, see Adding and editing an object list.\\nViewing results for groups of documents\\nYou can view and compare aiR for Review results for large groups of documents by adding their fields to document views and saved searches.\\nEach field name is formatted as aiR <review type> Analysis::<fieldname>. For example, the Prediction field for a Relevance analysis is called aiR Relevance Analysis::Prediction.\\nFor a full field list, see aiR for Review results fields.\\nNotes: \\nIf you run a new job on documents that were part of a previous job, you may temporarily see both sets of results linked to those documents. The old results will be unlinked after the new job is complete.\\n\\nTo avoid seeing doubled results, clear the previous result set using the aiR for Review Jobs tab.\\n\\n\\nCreating an aiR for Review results view\\nWhen creating a view for aiR for Review results, we recommend including these fields:\\n\\n\\nEdit\\n\\n\\nControl Number\\n\\n\\n<Review Field>\\n\\n\\naiR <Review Type> Analysis::Rank\\n\\n\\naiR <Review Type> Analysis::Prediction\\n\\n\\nBecause the Rationale, Citation, and Considerations fields have larger blocks of text, those tend to be less helpful for comparing many documents. However, you can also add those if desired.\\nFor a full field list, see aiR for Review results fields.\\nFiltering and sorting aiR for Review results\\nDocuments have a one-to-many relationship with the aiR for Review\\'s results fields. For example, a single document might be linked to five Issue results. This creates some limitations when sorting and filtering results:\\n\\n\\nFilter one column at a time in the Document list. Combining filters may include more results than you expect.\\n\\n\\nIf you need to filter by more than one field at a time, we recommend using search conditions instead.\\n\\n\\nYou can add these fields to views and widgets, but you cannot sort the view or the widget by these fields.\\n\\n\\naiR for Review results fields\\nThe results of every aiR for Review analysis are stored as part of an analysis object. Each of the three result types has its own object type to match:\\n\\n\\naiR Relevance Analysis\\n\\n\\naiR Key Analysis\\n\\n\\naiR Issue Analysis\\n\\n\\naiR also links the results to each of the documents that were analyzed. These linked fields, called reflected fields, update to link to the newest results every time the document is analyzed. However, aiR keeps a record of all previous job results, and you can link the documents to a different job at any time. For more information, see Managing jobs and document linking.\\nThe reflected fields are the most useful for reviewing analysis results. These are formatted as aiR <review type> Analysis::<fieldname>. For example, the Prediction field for a Relevance analysis is called aiR Relevance Analysis::Prediction.\\naiR Relevance Analysis fields\\nThe fields for aiR Relevance Analysis are:\\n\\n\\n\\n\\n\\n\\nField name\\nField type\\nDescription\\n\\n\\n\\n\\n\\nName\\n\\n\\nFixed-length Text\\n\\n\\nThe name of this specific result. This formatted as <Document Artifact ID>_<Job ID>.\\n\\n\\n\\n\\nJob ID\\n\\n\\nFixed-length Text\\n\\n\\nThe unique ID of the job this result came from.\\n\\n\\n\\nRank\\nWhole Number\\nNumerical rank indicating how strongly relevant the document is. For more information, see Understanding document ranks.\\n\\n\\nDocument\\nMultiple Object\\nThe Control Number of the document this result is linked to. If the result is not currently linked to any documents, this field is blank.\\n\\n\\nPrediction\\nFixed-length Text\\naiR\\'s prediction of whether this qualifies as a relevant document.\\n\\n\\nRationale\\nFixed-length Text\\nAn explanation of why aiR chose this rank and prediction.\\n\\n\\nConsiderations\\nFixed-length Text\\nA counterargument explaining why the rationale might possibly be wrong.\\n\\n\\nCitation 1\\nFixed-length Text\\nExcerpt from the document that supports the prediction and rationale. This may be blank for some documents.\\n\\n\\nCitation 2\\nFixed-length Text\\nSecond excerpt from the document that supports the prediction and rationale. This may be blank for some documents.\\n\\n\\nCitation 3\\nFixed-length Text\\nThird excerpt from the document that supports the prediction and rationale. This may be blank for some documents.\\n\\n\\nCitation 4\\nFixed-length Text\\nFourth excerpt from the document that supports the prediction and rationale. This may be blank for some documents.\\n\\n\\nCitation 5\\nFixed-length Text\\nFifth excerpt from the document that supports the prediction and rationale. This may be blank for some documents.\\n\\n\\nError Details\\nFixed-length Text\\nIf the document encountered an error, the error message displays here. For an error list, see How document errors are handled.\\n\\n\\n\\naiR Issues Analysis fields\\nThe fields for aiR Issues Analysis are:\\n\\n\\n\\n\\n\\n\\nField name\\nField type\\nDescription\\n\\n\\n\\n\\n\\nName\\n\\n\\nFixed-length Text\\n\\n\\nThe name of this specific result. This formatted as <Document ID>_<Job ID>.\\n\\n\\n\\n\\nJob ID\\n\\n\\nFixed-length Text\\n\\n\\nThe unique ID of the job this result came from.\\n\\n\\n\\n\\nChoice Analyzed\\n\\n\\nFixed Text\\n\\n\\nThe name of the issue choice being analyzed for this result.\\n\\n\\n\\n\\nChoice Analyzed ID\\n\\n\\nWhole Number\\n\\n\\nThe Artifact ID of the issue choice being analyzed for this\\r\\n result.\\n\\n\\n\\n\\nDocument\\n\\n\\nMultiple Object\\n\\n\\nThe Control Number of the document this result is linked to. If the result is not currently linked to any documents, this field is blank.\\n\\n\\n\\n\\nRank\\n\\n\\nWhole Number\\n\\n\\nNumerical rank indicating how well the document matches an issue. For more information, see Understanding document ranks.\\n\\n\\n\\n\\nPrediction\\n\\n\\nFixed-length Text\\n\\n\\naiR\\'s predicted issue choice for this document.\\n\\n\\n\\nRationale\\nFixed-length Text\\nAn explanation of why aiR chose this rank and prediction.\\n\\n\\nConsiderations\\nFixed-length Text\\nA counterargument explaining why the rationale might possibly be wrong.\\n\\n\\n\\nCitation\\n\\n\\nFixed-length Text\\n\\n\\nExcerpt from the document that supports the prediction and rationale. This may be blank for some documents.\\n\\n\\n\\n\\nError Details\\n\\n\\nFixed-length Text\\n\\n\\nIf the document encountered an error, the error message displays here. For an error list, see How document errors are handled.\\n\\n\\n\\n\\naiR Key Analysis fields\\nThe fields for aiR Key Analysis are:\\n\\n\\n\\n\\n\\n\\nField name\\nField type\\nDescription\\n\\n\\n\\n\\n\\nName\\n\\n\\nFixed-length Text\\n\\n\\nThe name of this specific result. This formatted as <Document ID>_<Job ID>.\\n\\n\\n\\n\\nJob ID\\n\\n\\nFixed-length Text\\n\\n\\nThe unique ID of the job this result came from.\\n\\n\\n\\n\\nDocument\\n\\n\\nMultiple Object\\n\\n\\nThe Control Number of the document this result is linked to. If the result is not currently linked to any documents, this field is blank.\\n\\n\\n\\n\\nRank\\n\\n\\nWhole Number\\n\\n\\nNumerical rank indicating how strongly relevant the document is. For more information, see Understanding document ranks.\\n\\n\\n\\n\\nPrediction\\n\\n\\nFixed-length Text\\n\\n\\naiR\\'s prediction of whether this qualifies as a key document.\\n\\n\\n\\nRationale\\nFixed-length Text\\nAn explanation of why aiR chose this rank and prediction.\\n\\n\\nConsiderations\\nFixed-length Text\\nA counterargument explaining why the rationale might possibly be wrong.\\n\\n\\nCitation 1\\nFixed-length Text\\nExcerpt from the document that supports the prediction and rationale. This may be blank for some documents.\\n\\n\\nCitation 2\\nFixed-length Text\\nSecond excerpt from the document that supports the prediction and rationale. This may be blank for some documents.\\n\\n\\nCitation 3\\nFixed-length Text\\nThird excerpt from the document that supports the prediction and rationale. This may be blank for some documents.\\n\\n\\nCitation 4\\nFixed-length Text\\nFourth excerpt from the document that supports the prediction and rationale. This may be blank for some documents.\\n\\n\\nCitation 5\\nFixed-length Text\\nFifth excerpt from the document that supports the prediction and rationale. This may be blank for some documents.\\n\\n\\n\\nError Details\\n\\n\\nFixed-length Text\\n\\n\\nIf the document encountered an error, the error message displays here. For an error list, see How document errors are handled.\\n\\n\\n\\n\\nUsing aiR for Review with Review Center\\nOne option for integrating aiR for Review into a larger review workflow is to combine it with Review Center. After analyzing the documents with aiR for Review, you can use aiR\\'s predictions to prioritize which documents to include in a Review Center queue.\\nFor example, you may want to review all documents that aiR for Review ranked as borderline or above for relevance. To do that:\\n\\n\\nSet up a saved search for documents where aiR Relevance Analysis::Rank is greater than 1. This returns all documents ranked 2 or higher.\\n\\n\\nCreate a Review Center queue using that saved search as the data source.\\n\\n\\nBecause of how the aiR for Review fields are structured, you cannot sort by them. However, you can either sort by another field, or use a prioritized review queue to dynamically serve up documents that may be most relevant.\\nFor more information, see Review Center.\\nHow document errors are handled\\nIf aiR encounters a problem when analyzing a document, it will not return results for that document. Instead, it ranks the document as -1 and returns an error message in the Error Details column. Your organization is not charged for any errored documents.\\nThe possible error messages are:\\n\\n\\n\\n\\n\\nError message\\nDescription\\n\\n\\n\\n\\nFailed to parse completion \\nThe large language model (LLM) encountered an error. \\n\\n\\nCompletion is not valid JSON \\nThe large language model (LLM) encountered an error.\\n\\n\\nHallucination detected in completion \\nThe results for this document may include a hallucination. For more information, see Hallucinations and conflations.\\n\\n\\nConflation detected in completion \\nThe results for this document may include a conflation. For more information, see Hallucinations and conflations.\\n\\n\\nDocument text is empty \\nThe extracted text of the document was empty.\\n\\n\\nDocument text is too short \\nThere was not enough extracted text to analyze in the document.\\n\\n\\nDocument text is too long \\nThe document\\'s extracted text was too long to analyze.\\n\\n\\nModel API error occurred \\nA communication error occurred between the large language model (LLM) and Relativity.\\n\\n\\nUncategorized error occurred \\nAn unknown error occurred.\\n\\n\\n\\nHallucinations and conflations\\nTwo types of errors deserve special mention:\\n\\n\\nHallucinations—these occur when the aiR results citation cannot be found anywhere in the prompt text. The large language model (LLM) appears to be citing sentences that don\\'t exist in the prompt.\\n\\n\\nConflations—these occur when the aiR results citation comes from something other than the document itself, but which is still part of the full prompt. For example, it might cite text that was part of the Prompt Criteria instead of the document\\'s extracted text.\\n\\n\\nWhen aiR receives the analysis results from the LLM, it checks all citations against the prompt text. Any possible hallucinations or conflations are marked as errors, and they receive a rank of -1 instead of whatever rank they were originally assigned. We recommend manually reviewing errored documents.\\nHallucinations are typically rare. However, highly structured documents such as Excel spreadsheets and PDF forms have a higher hallucination rate than other document types.\\nNote: Due to the way that columns of text are scanned in OCR, OCR’d documents are occasionally marked as hallucinations when the citations do actually exist in the original document.\\n\\naiR for Review security permissions\\nThis page contains information on the security permissions required for interacting with aiR for Review. For more information on setting permissions, see Workspace security.\\nNote: aiR for Review is currently in limited release. For information about the general release, contact your account representative.\\nSee these related pages:\\n\\naiR for Review\\n\\nRunning aiR for Review\\n\\nMonitoring aiR for Review jobs\\n\\naiR for Review results\\n\\n\\nRunning the aiR for Review mass action\\nTo run the aiR for Review mass action, you need the following permissions:\\n\\n\\n\\n\\n\\nObject Security\\nOther Settings\\n\\n\\n\\n\\n\\n\\naiR for Review Profile - View, Edit, Add\\n\\n\\n\\n\\nMass Operations - aiR for Review\\n\\n\\n\\n\\n\\nYou must also belong to at least one user group other than the Workspace Admin Group.\\nViewing the aiR for Review Jobs tab\\nThere are two versions of the aiR for Review Jobs tab: one at the instance level, and one at the workspace level. The instance-level tab shows all jobs across all workspaces, and it includes several extra columns to identify the workspace, matter, and client connected to each job.\\nThe following permissions allow users to see the job list and click on each job to view Prompt Criteria details. Users with access to this tab can also cancel in-progress jobs.\\nInstance-level permissions\\nTo view the instance-level aiR for Review Jobs tab, you need the following permissions:\\n\\n\\n\\n\\n\\nObject Security\\nTab Visibility \\n\\n\\n\\n\\n\\n\\nAdmin Repository - View\\naiR for Review Prompt Criteria - View\\n\\n\\n\\n\\naiR for Review Jobs\\n\\n\\n\\n\\n\\nAssign these permissions under the Instance Details tab.\\nViewing Prompt Criteria at the instance level\\nTo view Prompt Criteria details for a job, you also need some permissions within that job\\'s workspace:\\n\\nYou must belong to more than just the Workspace Admin Group within the workspace.\\nYou must have aiR for Review Prompt Criteria - View rights within that job\\'s workspace.\\n\\nIf you do not have these, you will be able to see jobs from that workspace, but you will not be able click on those jobs to view their Prompt Criteria.\\nYou can also use item-level permissions to restrict access to a specific job\\'s aiR for Review Prompt Criteria. For more information, see Levels of Security in Relativity.\\nWorkspace-level permissions\\nTo view the workspace-level aiR for Review Jobs tab, you need the following permissions:\\n\\n\\n\\n\\n\\nObject Security\\nTab Visibility \\n\\n\\n\\n\\n\\n\\naiR for Review Prompt Criteria - View\\n\\n\\n\\n\\naiR for Review Jobs\\n\\n\\n\\n\\n\\nAssign these permissions under the Workspace Details tab within the chosen workspace.\\nYou can also use item-level permissions to restrict access to a specific job\\'s aiR for Review Prompt Criteria. For more information, see Levels of Security in Relativity.\\nClearing and restoring job results\\nTo clear or restore job results using the aiR for Review Jobs tab, you need the following permissions:\\n\\n\\n\\n\\n\\nObject Security\\nTab Visibility \\n\\n\\n\\n\\n\\n\\nDocument - View, Edit\\naiR Relevance Analysis - View, Edit\\naiR Issue Analysis - View, Edit\\naiR Key Analysis - View, Edit\\n\\n\\n\\n\\naiR for Review Jobs\\n\\n\\n\\n\\n\\nIf you have Edit permissions for only one of the analysis types, you will only be able to clear or restore results of that type.\\nFor more information on clearing and restoring results, see Managing jobs and document linking.\\nViewing highlights in the Viewer\\nTo  see aiR for Review results highlighted in the Viewer, you need the following permissions:\\n\\n\\n\\n\\nObject Security\\n\\n\\n\\n\\n\\n\\naiR Relevance Analysis - View\\naiR Issue Analysis - View\\naiR Key Analysis - View\\n\\n\\n\\n\\n\\nIf you only have access to some of these, you will only see highlighting for those analysis types.\\n\\xa0\\n. What permissions are needed to run aiR for Review?'), type='text')], created_at=1715285745, file_ids=[], incomplete_at=None, incomplete_details=None, metadata={}, object='thread.message', role='user', run_id=None, status=None, thread_id='thread_OJKDp8jDlTbP228xdcxzLqdo')], object='list', first_id='msg_y43WaQNzHmVYjipTDbSWqlNo', last_id='msg_mdBAKl97CYSyvb9z5eWdY5Lc', has_more=False)\n"
     ]
    }
   ],
   "source": [
    "assistant = client.beta.assistants.create(\n",
    "  name=\"Relativity Tutor\",\n",
    "  instructions=\"You are a personal tutor for the RelativityOne application. Review documentation and answer questions to help me learn.\",\n",
    "  model=\"gpt-4-turbo\",\n",
    ")\n",
    "\n",
    "thread = client.beta.threads.create()\n",
    "\n",
    "message = client.beta.threads.messages.create(\n",
    "  thread_id=thread.id,\n",
    "  role=\"user\",\n",
    "  content=f\"I have a question about {llm_input}. What permissions are needed to run aiR for Review?\"\n",
    ")\n",
    "\n",
    "run = client.beta.threads.runs.create_and_poll(\n",
    "  thread_id=thread.id,\n",
    "  assistant_id=assistant.id,\n",
    "  instructions=\"Please address the user as Kelly. The user has a premium account.\"\n",
    ")\n",
    "\n",
    "if run.status == 'completed': \n",
    "  messages = client.beta.threads.messages.list(\n",
    "    thread_id=thread.id\n",
    "  )\n",
    "  print(messages)\n",
    "else:\n",
    "  print(run.status)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kw-ds-testing-dC1OrvnL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
